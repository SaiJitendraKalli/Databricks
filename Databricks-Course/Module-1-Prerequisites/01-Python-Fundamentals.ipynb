{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Fundamentals for Data Engineering\n",
    "\n",
    "## Overview\n",
    "This notebook covers essential Python concepts needed for Databricks data engineering. We'll refresh Python basics, functions, OOP, and key libraries.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master Python data structures\n",
    "- Understand functions and lambda expressions\n",
    "- Learn OOP basics for data engineering\n",
    "- Get familiar with essential libraries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python Data Structures Review\n",
    "\n",
    "Understanding Python's built-in data structures is crucial for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists - Ordered, mutable collection\n",
    "data_sources = ['CSV', 'JSON', 'Parquet', 'Delta']\n",
    "print(f\"Data sources: {data_sources}\")\n",
    "\n",
    "# List comprehension - efficient data transformation\n",
    "uppercase_sources = [source.upper() for source in data_sources]\n",
    "print(f\"Uppercase: {uppercase_sources}\")\n",
    "\n",
    "# Filtering with list comprehension\n",
    "filtered = [s for s in data_sources if len(s) > 4]\n",
    "print(f\"Sources with >4 chars: {filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries - Key-value pairs (like JSON)\n",
    "table_config = {\n",
    "    'name': 'customer_transactions',\n",
    "    'format': 'delta',\n",
    "    'partitions': ['year', 'month'],\n",
    "    'optimize': True\n",
    "}\n",
    "\n",
    "print(f\"Table name: {table_config['name']}\")\n",
    "print(f\"Partitions: {table_config.get('partitions', [])}\")\n",
    "\n",
    "# Dictionary comprehension\n",
    "config_upper = {k.upper(): v for k, v in table_config.items()}\n",
    "print(f\"Uppercase keys: {config_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuples - Immutable sequences (good for fixed configs)\n",
    "bronze_silver_gold = ('bronze', 'silver', 'gold')\n",
    "print(f\"Medallion layers: {bronze_silver_gold}\")\n",
    "\n",
    "# Sets - Unique elements (useful for deduplication)\n",
    "duplicate_ids = [1, 2, 2, 3, 4, 4, 5]\n",
    "unique_ids = set(duplicate_ids)\n",
    "print(f\"Unique IDs: {unique_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions and Lambda Expressions\n",
    "\n",
    "Functions help organize and reuse code. Lambda functions are useful for quick transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition with type hints\n",
    "def calculate_partition_key(date_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract year-month partition key from date string.\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date in format 'YYYY-MM-DD'\n",
    "    \n",
    "    Returns:\n",
    "        Partition key in format 'year=YYYY/month=MM'\n",
    "    \"\"\"\n",
    "    year, month, _ = date_str.split('-')\n",
    "    return f\"year={year}/month={month}\"\n",
    "\n",
    "# Test the function\n",
    "partition = calculate_partition_key('2024-03-15')\n",
    "print(f\"Partition: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with default arguments\n",
    "def create_table_path(database: str, table: str, layer: str = 'silver') -> str:\n",
    "    \"\"\"Generate Delta table path.\"\"\"\n",
    "    return f\"/mnt/{layer}/{database}/{table}\"\n",
    "\n",
    "print(create_table_path('retail', 'orders'))\n",
    "print(create_table_path('retail', 'orders', 'gold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda functions - anonymous functions for quick operations\n",
    "multiply_by_2 = lambda x: x * 2\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "doubled = list(map(multiply_by_2, numbers))\n",
    "print(f\"Doubled: {doubled}\")\n",
    "\n",
    "# Lambda with filter\n",
    "even_numbers = list(filter(lambda x: x % 2 == 0, numbers))\n",
    "print(f\"Even numbers: {even_numbers}\")\n",
    "\n",
    "# Lambda with sorted (useful for sorting data)\n",
    "files = [('data1.csv', 100), ('data2.csv', 50), ('data3.csv', 200)]\n",
    "sorted_files = sorted(files, key=lambda x: x[1], reverse=True)\n",
    "print(f\"Sorted by size: {sorted_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Object-Oriented Programming (OOP) Basics\n",
    "\n",
    "OOP helps structure complex data engineering workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Base class for data pipelines.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, source_path: str, target_path: str):\n",
    "        self.name = name\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "        self.status = 'initialized'\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"Extract data from source.\"\"\"\n",
    "        print(f\"[{self.name}] Extracting from {self.source_path}\")\n",
    "        self.status = 'extracted'\n",
    "        return self\n",
    "    \n",
    "    def transform(self):\n",
    "        \"\"\"Transform extracted data.\"\"\"\n",
    "        print(f\"[{self.name}] Transforming data\")\n",
    "        self.status = 'transformed'\n",
    "        return self\n",
    "    \n",
    "    def load(self):\n",
    "        \"\"\"Load transformed data to target.\"\"\"\n",
    "        print(f\"[{self.name}] Loading to {self.target_path}\")\n",
    "        self.status = 'completed'\n",
    "        return self\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the complete ETL pipeline.\"\"\"\n",
    "        self.extract().transform().load()\n",
    "        print(f\"[{self.name}] Pipeline completed with status: {self.status}\")\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = DataPipeline(\n",
    "    name='customer_etl',\n",
    "    source_path='/raw/customers.csv',\n",
    "    target_path='/silver/customers'\n",
    ")\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inheritance - extending base class\n",
    "class StreamingPipeline(DataPipeline):\n",
    "    \"\"\"Streaming version of data pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, source_path: str, target_path: str, checkpoint_path: str):\n",
    "        super().__init__(name, source_path, target_path)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.is_streaming = True\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"Extract streaming data.\"\"\"\n",
    "        print(f\"[{self.name}] Starting streaming from {self.source_path}\")\n",
    "        print(f\"[{self.name}] Using checkpoint: {self.checkpoint_path}\")\n",
    "        self.status = 'streaming'\n",
    "        return self\n",
    "\n",
    "# Create streaming pipeline\n",
    "streaming = StreamingPipeline(\n",
    "    name='event_stream',\n",
    "    source_path='/stream/events',\n",
    "    target_path='/silver/events',\n",
    "    checkpoint_path='/checkpoints/events'\n",
    ")\n",
    "streaming.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exception Handling\n",
    "\n",
    "Proper error handling is critical in production data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config_file(file_path: str) -> dict:\n",
    "    \"\"\"Read configuration file with error handling.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"Successfully loaded config from {file_path}\")\n",
    "        return config\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Config file {file_path} not found\")\n",
    "        # Return default config\n",
    "        return {'mode': 'default', 'retry': 3}\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON in {file_path}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        print(\"Config loading attempt completed\")\n",
    "\n",
    "# Test with non-existent file\n",
    "config = read_config_file('/tmp/nonexistent.json')\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Managers\n",
    "\n",
    "Context managers ensure proper resource cleanup (important for file handles, connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using context manager for file operations\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Create a temporary file\n",
    "temp_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')\n",
    "temp_path = temp_file.name\n",
    "temp_file.close()\n",
    "\n",
    "# Write with context manager - automatic cleanup\n",
    "with open(temp_path, 'w') as f:\n",
    "    f.write(\"customer_id,name,amount\\n\")\n",
    "    f.write(\"1,Alice,100\\n\")\n",
    "    f.write(\"2,Bob,200\\n\")\n",
    "\n",
    "# Read with context manager\n",
    "with open(temp_path, 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File content:\")\n",
    "    print(content)\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(temp_path)\n",
    "print(\"Temporary file cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Essential Libraries for Data Engineering\n",
    "\n",
    "These libraries are commonly used in Databricks data engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime - working with dates and times\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "now = datetime.now()\n",
    "print(f\"Current time: {now}\")\n",
    "print(f\"Formatted: {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Date arithmetic\n",
    "yesterday = now - timedelta(days=1)\n",
    "print(f\"Yesterday: {yesterday.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Parse date string\n",
    "date_str = \"2024-03-15\"\n",
    "parsed = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "print(f\"Parsed date: {parsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json - working with JSON data\n",
    "import json\n",
    "\n",
    "# Python dict to JSON\n",
    "pipeline_config = {\n",
    "    'name': 'sales_pipeline',\n",
    "    'schedule': '0 2 * * *',\n",
    "    'tables': ['orders', 'customers', 'products']\n",
    "}\n",
    "\n",
    "json_string = json.dumps(pipeline_config, indent=2)\n",
    "print(\"JSON output:\")\n",
    "print(json_string)\n",
    "\n",
    "# JSON to Python dict\n",
    "parsed_config = json.loads(json_string)\n",
    "print(f\"\\nParsed tables: {parsed_config['tables']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os and pathlib - file system operations\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Environment variables (common in Databricks)\n",
    "# os.environ.get('DATABRICKS_TOKEN', 'default_value')\n",
    "\n",
    "# Path operations\n",
    "base_path = Path('/dbfs/mnt/data')\n",
    "bronze_path = base_path / 'bronze' / 'customers'\n",
    "print(f\"Bronze path: {bronze_path}\")\n",
    "\n",
    "# Check if path exists (simulated)\n",
    "print(f\"Path object: {bronze_path}\")\n",
    "print(f\"Parent: {bronze_path.parent}\")\n",
    "print(f\"Name: {bronze_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. List Comprehensions and Generators\n",
    "\n",
    "Efficient data processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension - create lists efficiently\n",
    "numbers = range(1, 11)\n",
    "\n",
    "# Square all numbers\n",
    "squares = [n**2 for n in numbers]\n",
    "print(f\"Squares: {squares}\")\n",
    "\n",
    "# Conditional comprehension\n",
    "even_squares = [n**2 for n in numbers if n % 2 == 0]\n",
    "print(f\"Even squares: {even_squares}\")\n",
    "\n",
    "# Nested comprehension - flatten list of lists\n",
    "partitions = [['2024-01', '2024-02'], ['2024-03', '2024-04']]\n",
    "flattened = [month for sublist in partitions for month in sublist]\n",
    "print(f\"Flattened: {flattened}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator expressions - memory efficient for large datasets\n",
    "# Creates iterator instead of entire list in memory\n",
    "large_numbers = (n**2 for n in range(1000000))\n",
    "print(f\"Generator object: {large_numbers}\")\n",
    "\n",
    "# Use with next() or in loop\n",
    "print(f\"First value: {next(large_numbers)}\")\n",
    "print(f\"Second value: {next(large_numbers)}\")\n",
    "\n",
    "# Generator function with yield\n",
    "def partition_generator(start_year, end_year):\n",
    "    \"\"\"Generate year-month partitions.\"\"\"\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            yield f\"year={year}/month={month:02d}\"\n",
    "\n",
    "# Use generator\n",
    "partitions = partition_generator(2023, 2024)\n",
    "print(\"First 5 partitions:\")\n",
    "for i, partition in enumerate(partitions):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    print(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decorators\n",
    "\n",
    "Decorators modify or enhance functions (useful for logging, timing, caching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def timing_decorator(func):\n",
    "    \"\"\"Decorator to measure function execution time.\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timing_decorator\n",
    "def process_data(records_count):\n",
    "    \"\"\"Simulate data processing.\"\"\"\n",
    "    print(f\"Processing {records_count} records...\")\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "    return records_count * 2\n",
    "\n",
    "result = process_data(1000)\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_decorator(max_attempts=3):\n",
    "    \"\"\"Decorator to retry failed operations.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt}/{max_attempts} failed: {e}\")\n",
    "                    if attempt == max_attempts:\n",
    "                        raise\n",
    "                    time.sleep(1)  # Wait before retry\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@retry_decorator(max_attempts=3)\n",
    "def unstable_api_call(success_rate=0.7):\n",
    "    \"\"\"Simulate an unstable API call.\"\"\"\n",
    "    import random\n",
    "    if random.random() < success_rate:\n",
    "        return \"Success!\"\n",
    "    raise Exception(\"API call failed\")\n",
    "\n",
    "# This will retry up to 3 times if it fails\n",
    "try:\n",
    "    result = unstable_api_call(success_rate=0.3)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"All attempts failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Data Processing Function\n",
    "Create a function that processes a list of customer records (dicts) and returns only customers with purchases > $100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Your code here\n",
    "customers = [\n",
    "    {'id': 1, 'name': 'Alice', 'purchase_amount': 150},\n",
    "    {'id': 2, 'name': 'Bob', 'purchase_amount': 75},\n",
    "    {'id': 3, 'name': 'Charlie', 'purchase_amount': 200},\n",
    "    {'id': 4, 'name': 'David', 'purchase_amount': 90}\n",
    "]\n",
    "\n",
    "def filter_high_value_customers(customers, threshold=100):\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# high_value = filter_high_value_customers(customers)\n",
    "# print(high_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Pipeline Class\n",
    "Create a class `DataValidator` that validates data quality (checks for nulls, duplicates, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Your code here\n",
    "class DataValidator:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.issues = []\n",
    "    \n",
    "    def check_nulls(self):\n",
    "        # TODO: Check for None values\n",
    "        pass\n",
    "    \n",
    "    def check_duplicates(self):\n",
    "        # TODO: Check for duplicate entries\n",
    "        pass\n",
    "    \n",
    "    def report(self):\n",
    "        # TODO: Print validation report\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Python data structures (lists, dicts, sets, tuples)\n",
    "✅ Functions, lambda expressions, and decorators\n",
    "✅ OOP basics with classes and inheritance\n",
    "✅ Exception handling and context managers\n",
    "✅ Essential libraries (datetime, json, os, pathlib)\n",
    "✅ List comprehensions and generators\n",
    "✅ Practical patterns for data engineering\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the practice exercises\n",
    "2. Review any concepts that are unclear\n",
    "3. Move to [02-SQL-Essentials.ipynb](./02-SQL-Essentials.ipynb)\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Python Official Documentation](https://docs.python.org/3/)\n",
    "- [Real Python Tutorials](https://realpython.com/)\n",
    "- [Python for Data Analysis](https://wesmckinney.com/book/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
