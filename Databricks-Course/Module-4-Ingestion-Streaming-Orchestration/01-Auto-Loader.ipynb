{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Loader - Incremental File Ingestion\n",
    "\n",
    "## Overview\n",
    "This notebook covers Auto Loader (cloudFiles) - the recommended way to incrementally and efficiently ingest data from cloud storage into Delta Lake.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand Auto Loader architecture\n",
    "- Configure Auto Loader for different file formats\n",
    "- Handle schema inference and evolution\n",
    "- Process new files automatically\n",
    "- Handle errors and rescue data\n",
    "- Monitor Auto Loader streams\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Auto Loader Basics\n",
    "\n",
    "### What is Auto Loader?\n",
    "\n",
    "**Auto Loader** incrementally and efficiently processes new data files as they arrive in cloud storage.\n",
    "\n",
    "**Benefits**:\n",
    "- ✅ Scalable (handles millions of files)\n",
    "- ✅ Efficient (only processes new files)\n",
    "- ✅ Automatic schema inference and evolution\n",
    "- ✅ Built-in error handling\n",
    "- ✅ Exactly-once processing guarantees\n",
    "\n",
    "**Use Cases**:\n",
    "- Ingesting logs from S3/ADLS/GCS\n",
    "- Processing streaming data files\n",
    "- Building data lakes\n",
    "- ETL pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Auto Loader Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define paths\n",
    "source_path = \"/path/to/source/files\"\n",
    "checkpoint_path = \"/path/to/checkpoint\"\n",
    "target_path = \"/path/to/delta/table\"\n",
    "\n",
    "# Basic Auto Loader configuration\n",
    "df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path) \\\n",
    "    .load(source_path)\n",
    "\n",
    "# Write to Delta\n",
    "query = df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .start(target_path)\n",
    "\n",
    "print(\"Auto Loader stream configured\")\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. File Format Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Auto Loader\n",
    "json_stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/json\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .load(\"/source/json/*\")\n",
    "\n",
    "print(\"JSON Auto Loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Auto Loader\n",
    "csv_stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/source/csv/*\")\n",
    "\n",
    "print(\"CSV Auto Loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet Auto Loader\n",
    "parquet_stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"parquet\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/parquet\") \\\n",
    "    .load(\"/source/parquet/*\")\n",
    "\n",
    "print(\"Parquet Auto Loader configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schema Inference and Evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Schema Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto Loader automatically infers schema from sample files\n",
    "auto_schema = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/infer\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "print(\"Schema will be inferred automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"value\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "explicit_schema = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "print(\"Explicit schema applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable schema evolution\n",
    "evolving_schema = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/evolve\") \\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\") \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "# Schema evolution modes:\n",
    "# - addNewColumns: Add new columns (default)\n",
    "# - rescue: Put unknown columns in _rescued_data\n",
    "# - failOnNewColumns: Fail if schema changes\n",
    "\n",
    "print(\"Schema evolution enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. File Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include file metadata columns\n",
    "with_metadata = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/metadata\") \\\n",
    "    .option(\"cloudFiles.includeExistingFiles\", \"true\") \\\n",
    "    .load(\"/source/data/*\") \\\n",
    "    .select(\n",
    "        \"*\",\n",
    "        \"_metadata.file_path\",\n",
    "        \"_metadata.file_name\",\n",
    "        \"_metadata.file_size\",\n",
    "        \"_metadata.file_modification_time\"\n",
    "    )\n",
    "\n",
    "print(\"File metadata columns included\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Rescue Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescued Data Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescue malformed or unexpected data\n",
    "with_rescue = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/rescue\") \\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") \\\n",
    "    .option(\"rescuedDataColumn\", \"_rescued_data\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "# Unknown columns will be stored in _rescued_data\n",
    "print(\"Rescue data column enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Records Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write bad records to separate location\n",
    "with_bad_records = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/badrecords\") \\\n",
    "    .option(\"badRecordsPath\", \"/bad_records\") \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "print(\"Bad records path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Notification Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use file notification for better scalability\n",
    "# (Requires setup of cloud notifications)\n",
    "\n",
    "file_notification = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/notify\") \\\n",
    "    .option(\"cloudFiles.useNotifications\", \"true\") \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "# File notification modes:\n",
    "# - Directory listing (default): Lists directory to find files\n",
    "# - File notification: Uses cloud notifications (more scalable)\n",
    "\n",
    "print(\"File notification mode configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Files Per Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control processing rate\n",
    "rate_limited = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/rate\") \\\n",
    "    .option(\"maxFilesPerTrigger\", \"100\") \\\n",
    "    .load(\"/source/data/*\")\n",
    "\n",
    "print(\"Rate limiting configured: 100 files per trigger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Auto Loader Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete example: JSON to Delta with transformations\n",
    "\n",
    "# Read with Auto Loader\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"json\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/checkpoint/bronze\") \\\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") \\\n",
    "    .load(\"/source/events/*\")\n",
    "\n",
    "# Add metadata and transformations\n",
    "processed_stream = raw_stream \\\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"ingestion_date\", current_date()) \\\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Write to Bronze layer\n",
    "bronze_query = processed_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoint/bronze\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start(\"/delta/bronze/events\")\n",
    "\n",
    "print(\"Complete Auto Loader pipeline configured\")\n",
    "print(f\"Query ID: {bronze_query.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring Auto Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor stream status\n",
    "# print(f\"Is active: {bronze_query.isActive}\")\n",
    "# print(f\"Status: {bronze_query.status}\")\n",
    "\n",
    "# Get recent progress\n",
    "# recent = bronze_query.recentProgress\n",
    "# print(f\"Recent batches: {len(recent)}\")\n",
    "\n",
    "# Get last progress\n",
    "# last = bronze_query.lastProgress\n",
    "# if last:\n",
    "#     print(f\"Input rows: {last['numInputRows']}\")\n",
    "#     print(f\"Processed rows: {last['processedRowsPerSecond']}\")\n",
    "\n",
    "print(\"Monitoring methods available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice Guidelines\n",
    "\n",
    "1. **Schema Location**: Always specify `cloudFiles.schemaLocation`\n",
    "   ```python\n",
    "   .option(\"cloudFiles.schemaLocation\", \"/checkpoint/path\")\n",
    "   ```\n",
    "\n",
    "2. **Schema Evolution**: Enable for flexibility\n",
    "   ```python\n",
    "   .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "   ```\n",
    "\n",
    "3. **File Notification**: Use for large-scale ingestion\n",
    "   ```python\n",
    "   .option(\"cloudFiles.useNotifications\", \"true\")\n",
    "   ```\n",
    "\n",
    "4. **Rate Limiting**: Control processing rate\n",
    "   ```python\n",
    "   .option(\"maxFilesPerTrigger\", \"1000\")\n",
    "   ```\n",
    "\n",
    "5. **Checkpoint Management**: Use separate checkpoints per stream\n",
    "\n",
    "6. **Partitioning**: Partition by date for better organization\n",
    "   ```python\n",
    "   .partitionBy(\"date\")\n",
    "   ```\n",
    "\n",
    "7. **Metadata**: Include file metadata for traceability\n",
    "   ```python\n",
    "   .withColumn(\"source_file\", input_file_name())\n",
    "   ```\n",
    "\n",
    "8. **Error Handling**: Use rescue columns and bad records path\n",
    "   ```python\n",
    "   .option(\"badRecordsPath\", \"/bad_records\")\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: CSV Ingestion\n",
    "Create an Auto Loader pipeline for CSV files with schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Configure Auto Loader for CSV with schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Bronze-to-Silver Pipeline\n",
    "Read from a bronze Delta table and write to silver with data quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Create bronze-to-silver pipeline with quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Auto Loader architecture and benefits\n",
    "✅ Configuration for different file formats\n",
    "✅ Schema inference and evolution\n",
    "✅ File metadata tracking\n",
    "✅ Error handling with rescue columns\n",
    "✅ Performance optimization techniques\n",
    "✅ Complete pipeline examples\n",
    "✅ Monitoring and best practices\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Implement Auto Loader in your projects\n",
    "2. Set up file notifications for scale\n",
    "3. Build medallion architecture pipelines\n",
    "4. Learn about Delta Live Tables\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Auto Loader Documentation](https://docs.databricks.com/ingestion/auto-loader/index.html)\n",
    "- [Schema Evolution](https://docs.databricks.com/ingestion/auto-loader/schema.html)\n",
    "- [Spark By Examples - Auto Loader](https://sparkbyexamples.com/pyspark/pyspark-autoloader/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
