{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Joins and Performance Optimization\n",
    "\n",
    "## Overview\n",
    "This notebook covers all types of joins in PySpark, union operations, and performance optimization techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master all join types (inner, outer, left, right, cross, semi, anti)\n",
    "- Understand join strategies and performance\n",
    "- Use broadcast joins effectively\n",
    "- Perform union and union all operations\n",
    "- Optimize DataFrame operations\n",
    "- Handle data skew\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Join Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Sample data - Customers\n",
    "customers_data = [\n",
    "    (1, \"Alice\", \"NY\"),\n",
    "    (2, \"Bob\", \"CA\"),\n",
    "    (3, \"Charlie\", \"TX\"),\n",
    "    (4, \"Diana\", \"FL\")\n",
    "]\n",
    "\n",
    "customers = spark.createDataFrame(\n",
    "    customers_data,\n",
    "    [\"customer_id\", \"name\", \"state\"]\n",
    ")\n",
    "\n",
    "# Sample data - Orders\n",
    "orders_data = [\n",
    "    (101, 1, 150.0, \"2024-01-15\"),\n",
    "    (102, 1, 200.0, \"2024-01-16\"),\n",
    "    (103, 2, 75.0, \"2024-01-17\"),\n",
    "    (104, 3, 300.0, \"2024-01-18\"),\n",
    "    (105, 5, 120.0, \"2024-01-19\")  # customer_id 5 doesn't exist\n",
    "]\n",
    "\n",
    "orders = spark.createDataFrame(\n",
    "    orders_data,\n",
    "    [\"order_id\", \"customer_id\", \"amount\", \"order_date\"]\n",
    ")\n",
    "\n",
    "print(\"Customers:\")\n",
    "display(customers)\n",
    "\n",
    "print(\"\\nOrders:\")\n",
    "display(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "Returns only matching rows from both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join - only matching records\n",
    "inner_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    customers.customer_id,\n",
    "    customers.name,\n",
    "    orders.order_id,\n",
    "    orders.amount\n",
    ")\n",
    "\n",
    "print(\"Inner Join Result:\")\n",
    "display(inner_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join\n",
    "Returns all rows from left DataFrame, with nulls for non-matching right rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - all customers, even without orders\n",
    "left_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"left\"\n",
    ").select(\n",
    "    customers.customer_id,\n",
    "    customers.name,\n",
    "    orders.order_id,\n",
    "    orders.amount\n",
    ")\n",
    "\n",
    "print(\"Left Join Result (all customers):\")\n",
    "display(left_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join\n",
    "Returns all rows from right DataFrame, with nulls for non-matching left rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - all orders, even without matching customer\n",
    "right_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"right\"\n",
    ").select(\n",
    "    orders.order_id,\n",
    "    orders.customer_id.alias(\"order_customer_id\"),\n",
    "    customers.customer_id.alias(\"cust_id\"),\n",
    "    customers.name,\n",
    "    orders.amount\n",
    ")\n",
    "\n",
    "print(\"Right Join Result (all orders):\")\n",
    "display(right_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Outer Join\n",
    "Returns all rows from both DataFrames, with nulls where there's no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full outer join - all customers and all orders\n",
    "full_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"full_outer\"\n",
    ").select(\n",
    "    coalesce(customers.customer_id, orders.customer_id).alias(\"customer_id\"),\n",
    "    customers.name,\n",
    "    orders.order_id,\n",
    "    orders.amount\n",
    ")\n",
    "\n",
    "print(\"Full Outer Join Result:\")\n",
    "display(full_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Join\n",
    "Cartesian product - every row from left with every row from right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross join - Cartesian product (use carefully!)\n",
    "cross_join = customers.crossJoin(orders).select(\n",
    "    customers.customer_id.alias(\"cust_id\"),\n",
    "    customers.name,\n",
    "    orders.order_id,\n",
    "    orders.customer_id.alias(\"order_cust_id\")\n",
    ")\n",
    "\n",
    "print(f\"Cross Join Result ({customers.count()} × {orders.count()} = {cross_join.count()} rows):\")\n",
    "display(cross_join.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Semi Join\n",
    "Returns rows from left DataFrame that have a match in right (like EXISTS in SQL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left semi join - customers who have placed orders\n",
    "semi_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"left_semi\"\n",
    ")\n",
    "\n",
    "print(\"Left Semi Join (customers with orders):\")\n",
    "display(semi_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Anti Join\n",
    "Returns rows from left DataFrame that DON'T have a match in right (like NOT EXISTS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left anti join - customers without orders\n",
    "anti_join = customers.join(\n",
    "    orders,\n",
    "    customers.customer_id == orders.customer_id,\n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "print(\"Left Anti Join (customers without orders):\")\n",
    "display(anti_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Join Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with multiple join keys\n",
    "products_data = [\n",
    "    (1, \"A\", \"Product 1\"),\n",
    "    (1, \"B\", \"Product 2\"),\n",
    "    (2, \"A\", \"Product 3\"),\n",
    "    (2, \"B\", \"Product 4\")\n",
    "]\n",
    "\n",
    "products = spark.createDataFrame(\n",
    "    products_data,\n",
    "    [\"category_id\", \"region\", \"product_name\"]\n",
    ")\n",
    "\n",
    "sales_data = [\n",
    "    (1, \"A\", 100),\n",
    "    (1, \"B\", 150),\n",
    "    (2, \"A\", 200)\n",
    "]\n",
    "\n",
    "sales = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"category_id\", \"region\", \"sales_amount\"]\n",
    ")\n",
    "\n",
    "# Join on multiple conditions\n",
    "multi_join = products.join(\n",
    "    sales,\n",
    "    (products.category_id == sales.category_id) & \n",
    "    (products.region == sales.region),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    products.product_name,\n",
    "    products.region,\n",
    "    sales.sales_amount\n",
    ")\n",
    "\n",
    "print(\"Multi-condition Join:\")\n",
    "display(multi_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Broadcast Joins (Performance Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# When one DataFrame is small (<10MB), use broadcast join\n",
    "# This avoids shuffling the large DataFrame\n",
    "\n",
    "broadcast_join = orders.join(\n",
    "    broadcast(customers),  # Broadcast small table\n",
    "    \"customer_id\",\n",
    "    \"inner\"\n",
    ").select(\n",
    "    customers.name,\n",
    "    orders.order_id,\n",
    "    orders.amount\n",
    ")\n",
    "\n",
    "print(\"Broadcast Join (optimized):\")\n",
    "display(broadcast_join)\n",
    "\n",
    "# Check execution plan\n",
    "print(\"\\nExecution Plan:\")\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employee-Manager relationship\n",
    "employees_data = [\n",
    "    (1, \"Alice\", None),\n",
    "    (2, \"Bob\", 1),\n",
    "    (3, \"Charlie\", 1),\n",
    "    (4, \"Diana\", 2),\n",
    "    (5, \"Eve\", 2)\n",
    "]\n",
    "\n",
    "employees = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"emp_name\", \"manager_id\"]\n",
    ")\n",
    "\n",
    "# Self join to get manager names\n",
    "emp_alias = employees.alias(\"emp\")\n",
    "mgr_alias = employees.alias(\"mgr\")\n",
    "\n",
    "self_join = emp_alias.join(\n",
    "    mgr_alias,\n",
    "    col(\"emp.manager_id\") == col(\"mgr.emp_id\"),\n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"emp.emp_name\").alias(\"employee\"),\n",
    "    col(\"mgr.emp_name\").alias(\"manager\")\n",
    ")\n",
    "\n",
    "print(\"Self Join (Employee-Manager):\")\n",
    "display(self_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Union Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data - Q1 and Q2 sales\n",
    "q1_sales = spark.createDataFrame(\n",
    "    [(1, \"Product A\", 100), (2, \"Product B\", 150)],\n",
    "    [\"id\", \"product\", \"amount\"]\n",
    ")\n",
    "\n",
    "q2_sales = spark.createDataFrame(\n",
    "    [(2, \"Product B\", 150), (3, \"Product C\", 200)],  # Note: duplicate row\n",
    "    [\"id\", \"product\", \"amount\"]\n",
    ")\n",
    "\n",
    "# Union (removes duplicates) - deprecated, use distinct after unionAll\n",
    "union_result = q1_sales.union(q2_sales)\n",
    "print(f\"Union (with duplicates): {union_result.count()} rows\")\n",
    "display(union_result)\n",
    "\n",
    "# UnionAll (keeps duplicates) - same as union in Spark 3.x\n",
    "union_all_result = q1_sales.unionAll(q2_sales)\n",
    "print(f\"\\nUnion All: {union_all_result.count()} rows\")\n",
    "display(union_all_result)\n",
    "\n",
    "# Remove duplicates\n",
    "distinct_result = union_all_result.distinct()\n",
    "print(f\"\\nDistinct after Union: {distinct_result.count()} rows\")\n",
    "display(distinct_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union by Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames with different column orders\n",
    "df1 = spark.createDataFrame(\n",
    "    [(1, \"A\", 100)],\n",
    "    [\"id\", \"name\", \"value\"]\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(200, 2, \"B\")],\n",
    "    [\"value\", \"id\", \"name\"]  # Different order!\n",
    ")\n",
    "\n",
    "# unionByName - matches by column name, not position\n",
    "union_by_name = df1.unionByName(df2)\n",
    "print(\"Union By Name (handles different column orders):\")\n",
    "display(union_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache DataFrame for reuse\n",
    "customers_cached = customers.cache()\n",
    "\n",
    "# Use cached DataFrame multiple times\n",
    "count1 = customers_cached.count()\n",
    "count2 = customers_cached.filter(col(\"state\") == \"NY\").count()\n",
    "\n",
    "# Unpersist when done\n",
    "customers_cached.unpersist()\n",
    "\n",
    "print(f\"Total customers: {count1}\")\n",
    "print(f\"NY customers: {count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current partitions\n",
    "print(f\"Current partitions: {orders.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition (increases or decreases partitions with shuffle)\n",
    "orders_repart = orders.repartition(4)\n",
    "print(f\"After repartition: {orders_repart.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by column (for better data locality)\n",
    "orders_by_customer = orders.repartition(4, \"customer_id\")\n",
    "print(f\"Repartitioned by customer_id: {orders_by_customer.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (only decreases partitions, no shuffle)\n",
    "orders_coal = orders.coalesce(2)\n",
    "print(f\"After coalesce: {orders_coal.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Filter before join (reduces data to join)\n",
    "high_value_orders = orders.filter(col(\"amount\") > 100)\n",
    "result_optimized = customers.join(high_value_orders, \"customer_id\")\n",
    "\n",
    "# Less efficient: Filter after join\n",
    "result_unoptimized = customers.join(orders, \"customer_id\").filter(col(\"amount\") > 100)\n",
    "\n",
    "print(\"Filter before join (optimized):\")\n",
    "display(result_optimized)\n",
    "\n",
    "# Both produce same result, but first is more efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good: Select only needed columns early\n",
    "customers_subset = customers.select(\"customer_id\", \"name\")\n",
    "result = orders.join(customers_subset, \"customer_id\")\n",
    "\n",
    "# Less efficient: Select all columns, then filter\n",
    "# (Spark optimizer often handles this, but good practice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Data Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data skew: when one key has many more values than others\n",
    "\n",
    "# Technique 1: Salt the key\n",
    "# Add random suffix to distribute skewed key\n",
    "\n",
    "skewed_df = orders.withColumn(\n",
    "    \"salted_key\",\n",
    "    concat(col(\"customer_id\"), lit(\"_\"), (rand() * 10).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Technique 2: Broadcast the smaller table\n",
    "# Already covered above\n",
    "\n",
    "# Technique 3: Use AQE (Adaptive Query Execution)\n",
    "# spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "print(\"Salted keys for skew handling:\")\n",
    "display(skewed_df.select(\"customer_id\", \"salted_key\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explain Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View physical plan\n",
    "join_query = customers.join(orders, \"customer_id\")\n",
    "\n",
    "print(\"Physical Plan:\")\n",
    "join_query.explain()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Extended Explanation:\")\n",
    "join_query.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Customer Purchase Analysis\n",
    "Find customers who placed orders in both Q1 and Q2 (use semi join or intersect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Create Q1 and Q2 order datasets, find customers in both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimize a Join\n",
    "Given a large orders table and small products table, write an optimized join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Use broadcast join for optimal performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ All join types (inner, left, right, full, cross, semi, anti)\n",
    "✅ Multiple join conditions\n",
    "✅ Broadcast joins for performance\n",
    "✅ Self joins\n",
    "✅ Union and unionByName operations\n",
    "✅ Performance optimization (caching, repartitioning, filter pushdown)\n",
    "✅ Handling data skew\n",
    "✅ Reading explain plans\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Practice with larger datasets\n",
    "2. Monitor query plans and optimize\n",
    "3. Learn about Adaptive Query Execution (AQE)\n",
    "4. Study join strategies in depth\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Spark Join Strategies](https://sparkbyexamples.com/spark/spark-sql-join-types/)\n",
    "- [Performance Tuning Guide](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n",
    "- [Join Performance](https://sparkbyexamples.com/pyspark-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
