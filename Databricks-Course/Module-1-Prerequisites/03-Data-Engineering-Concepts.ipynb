{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Core Concepts\n",
    "\n",
    "## Overview\n",
    "This notebook introduces fundamental data engineering concepts essential for working with Databricks. We'll cover architecture patterns, data processing paradigms, and best practices.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand batch vs streaming processing\n",
    "- Learn data storage architectures (warehouse, lake, lakehouse)\n",
    "- Master partitioning strategies\n",
    "- Implement Slowly Changing Dimensions (SCD)\n",
    "- Understand Medallion architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Batch vs Streaming Processing\n",
    "\n",
    "Understanding when to use batch or streaming is critical for data engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing\n",
    "\n",
    "**Definition**: Processing large volumes of data at scheduled intervals.\n",
    "\n",
    "**Use Cases**:\n",
    "- Daily/weekly/monthly reports\n",
    "- Historical data analysis\n",
    "- Data warehouse ETL\n",
    "- ML model training on historical data\n",
    "\n",
    "**Example Workflow**:\n",
    "```\n",
    "Extract (source) → Transform (business logic) → Load (target)\n",
    "Runs: Every night at 2 AM\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Simple to implement and debug\n",
    "- Cost-effective for large volumes\n",
    "- Can process entire dataset\n",
    "- Easier to maintain\n",
    "\n",
    "**Disadvantages**:\n",
    "- High latency (hours to process)\n",
    "- Not suitable for real-time needs\n",
    "- Resource intensive during batch window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Processing Example (Pseudo-code)\n",
    "\n",
    "batch_example = \"\"\"\n",
    "# Daily batch job - runs at 2 AM\n",
    "def daily_sales_etl():\n",
    "    # Extract: Read yesterday's data\n",
    "    raw_sales = spark.read.parquet(\n",
    "        f\"/bronze/sales/date={yesterday}\"\n",
    "    )\n",
    "    \n",
    "    # Transform: Apply business logic\n",
    "    cleaned_sales = (\n",
    "        raw_sales\n",
    "        .filter(\"amount > 0\")\n",
    "        .withColumn(\"revenue\", col(\"quantity\") * col(\"price\"))\n",
    "        .groupBy(\"product_id\", \"region\")\n",
    "        .agg(\n",
    "            sum(\"revenue\").alias(\"total_revenue\"),\n",
    "            count(\"*\").alias(\"order_count\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Load: Write to gold layer\n",
    "    cleaned_sales.write.mode(\"append\").partitionBy(\"date\").save(\n",
    "        \"/gold/daily_sales\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Processed {raw_sales.count()} records\")\n",
    "\"\"\"\n",
    "\n",
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Processing\n",
    "\n",
    "**Definition**: Continuous processing of data as it arrives.\n",
    "\n",
    "**Use Cases**:\n",
    "- Real-time dashboards\n",
    "- Fraud detection\n",
    "- IoT sensor monitoring\n",
    "- Clickstream analysis\n",
    "- Real-time alerts\n",
    "\n",
    "**Example Workflow**:\n",
    "```\n",
    "Stream Source → Micro-batch Processing → Stream Sink\n",
    "Runs: Continuously (every few seconds)\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Low latency (seconds to minutes)\n",
    "- Immediate insights\n",
    "- Handles continuous data\n",
    "- Better resource utilization\n",
    "\n",
    "**Disadvantages**:\n",
    "- More complex to implement\n",
    "- Harder to debug\n",
    "- Requires careful state management\n",
    "- Higher operational overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Processing Example (Pseudo-code)\n",
    "\n",
    "streaming_example = \"\"\"\n",
    "# Real-time fraud detection stream\n",
    "def fraud_detection_stream():\n",
    "    # Read stream from Kafka/Event Hub\n",
    "    transactions_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"subscribe\", \"transactions\")\n",
    "        .load()\n",
    "    )\n",
    "    \n",
    "    # Transform: Apply fraud rules in real-time\n",
    "    fraud_checks = (\n",
    "        transactions_stream\n",
    "        .withColumn(\"is_suspicious\", \n",
    "            (col(\"amount\") > 10000) | \n",
    "            (col(\"location_change_speed\") > 500)\n",
    "        )\n",
    "        .filter(col(\"is_suspicious\"))\n",
    "    )\n",
    "    \n",
    "    # Write stream to Delta table\n",
    "    query = (\n",
    "        fraud_checks.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/checkpoints/fraud\")\n",
    "        .start(\"/alerts/fraud_detected\")\n",
    "    )\n",
    "    \n",
    "    return query\n",
    "\"\"\"\n",
    "\n",
    "print(streaming_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Batch vs Streaming\n",
    "\n",
    "| Factor | Batch | Streaming |\n",
    "|--------|-------|----------|\n",
    "| **Latency** | Hours to days | Seconds to minutes |\n",
    "| **Complexity** | Low | High |\n",
    "| **Cost** | Lower (scheduled) | Higher (always on) |\n",
    "| **Use Case** | Historical analysis | Real-time insights |\n",
    "| **Data Volume** | Large volumes | Continuous flow |\n",
    "| **Debugging** | Easy | Challenging |\n",
    "\n",
    "**Hybrid Approach**: Lambda Architecture\n",
    "- Batch layer: Complete, accurate historical data\n",
    "- Speed layer: Real-time approximate data\n",
    "- Serving layer: Combines both views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Storage Architectures\n",
    "\n",
    "Evolution from data warehouses to modern lakehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warehouse\n",
    "\n",
    "**Traditional structured data storage**\n",
    "\n",
    "**Characteristics**:\n",
    "- Structured data only (tables, schemas)\n",
    "- SQL-based queries\n",
    "- ETL before storage (schema-on-write)\n",
    "- Expensive scaling\n",
    "- ACID transactions\n",
    "\n",
    "**Examples**: Snowflake, Redshift, BigQuery\n",
    "\n",
    "**Pros**:\n",
    "- ✅ Fast queries on structured data\n",
    "- ✅ Strong consistency\n",
    "- ✅ Well-established tools\n",
    "\n",
    "**Cons**:\n",
    "- ❌ Can't handle unstructured data\n",
    "- ❌ Expensive for large volumes\n",
    "- ❌ Limited flexibility\n",
    "\n",
    "```\n",
    "Data Warehouse Architecture:\n",
    "\n",
    "Source Systems → ETL → Data Warehouse → BI Tools\n",
    "                       (Structured only)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Lake\n",
    "\n",
    "**Store all data in raw format**\n",
    "\n",
    "**Characteristics**:\n",
    "- All data types (structured, semi-structured, unstructured)\n",
    "- Schema-on-read (define structure when reading)\n",
    "- Cost-effective storage (object storage)\n",
    "- No ACID guarantees initially\n",
    "\n",
    "**Examples**: S3, ADLS, GCS with Parquet/CSV/JSON\n",
    "\n",
    "**Pros**:\n",
    "- ✅ Handles any data type\n",
    "- ✅ Very cost-effective\n",
    "- ✅ Scalable storage\n",
    "- ✅ Good for ML/AI\n",
    "\n",
    "**Cons**:\n",
    "- ❌ No ACID transactions (traditional lakes)\n",
    "- ❌ Can become \"data swamp\"\n",
    "- ❌ Slower queries\n",
    "- ❌ Data quality issues\n",
    "\n",
    "```\n",
    "Data Lake Architecture:\n",
    "\n",
    "All Sources → Data Lake → Multiple Tools\n",
    "              (All types)  (Spark, Python, SQL, ML)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lakehouse (Databricks Architecture)\n",
    "\n",
    "**Best of both worlds: Warehouse + Lake**\n",
    "\n",
    "**Characteristics**:\n",
    "- Built on data lake (cost-effective storage)\n",
    "- ACID transactions (Delta Lake)\n",
    "- Schema enforcement + evolution\n",
    "- Unified for BI, ML, and streaming\n",
    "- Time travel, versioning\n",
    "\n",
    "**Technology**: Delta Lake on cloud storage\n",
    "\n",
    "**Pros**:\n",
    "- ✅ Cost of data lake\n",
    "- ✅ Performance of warehouse\n",
    "- ✅ ACID transactions\n",
    "- ✅ Handles all data types\n",
    "- ✅ Single platform for all workloads\n",
    "\n",
    "```\n",
    "Lakehouse Architecture:\n",
    "\n",
    "                    Lakehouse (Delta Lake)\n",
    "                           |\n",
    "                    Cloud Storage\n",
    "                  (S3, ADLS, GCS)\n",
    "                           |\n",
    "         ┌─────────────────┼─────────────────┐\n",
    "      BI Tools         ML/AI            Streaming\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "comparison = \"\"\"\n",
    "| Feature | Warehouse | Lake | Lakehouse |\n",
    "|---------|-----------|------|------------|\n",
    "| Data Types | Structured | All | All |\n",
    "| ACID | Yes | No | Yes |\n",
    "| Cost | High | Low | Low |\n",
    "| Performance | High | Medium | High |\n",
    "| Schema | On-write | On-read | Both |\n",
    "| BI | Excellent | Poor | Excellent |\n",
    "| ML/AI | Limited | Excellent | Excellent |\n",
    "| Governance | Strong | Weak | Strong |\n",
    "| Examples | Snowflake | S3+Parquet | Delta Lake |\n",
    "\"\"\"\n",
    "\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partitioning Strategies\n",
    "\n",
    "Partitioning organizes data for efficient querying and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Partition?\n",
    "\n",
    "**Benefits**:\n",
    "1. **Query Performance**: Skip irrelevant data\n",
    "2. **Parallel Processing**: Process partitions in parallel\n",
    "3. **Data Management**: Easy to update/delete specific partitions\n",
    "4. **Cost**: Read less data = lower cost\n",
    "\n",
    "### Common Partitioning Strategies\n",
    "\n",
    "#### 1. Date/Time Partitioning (Most Common)\n",
    "```python\n",
    "# Partition by date\n",
    "/data/transactions/\n",
    "    date=2024-01-01/\n",
    "    date=2024-01-02/\n",
    "    date=2024-01-03/\n",
    "\n",
    "# Hierarchical date partitioning\n",
    "/data/transactions/\n",
    "    year=2024/\n",
    "        month=01/\n",
    "            day=01/\n",
    "```\n",
    "\n",
    "**Best for**: Time-series data, logs, events\n",
    "**Query example**: `WHERE date >= '2024-01-01'`\n",
    "\n",
    "#### 2. Category Partitioning\n",
    "```python\n",
    "/data/sales/\n",
    "    region=US/\n",
    "    region=EU/\n",
    "    region=APAC/\n",
    "```\n",
    "\n",
    "**Best for**: Geographic data, product categories\n",
    "**Query example**: `WHERE region = 'US'`\n",
    "\n",
    "#### 3. Hybrid Partitioning\n",
    "```python\n",
    "/data/orders/\n",
    "    year=2024/\n",
    "        month=01/\n",
    "            region=US/\n",
    "            region=EU/\n",
    "```\n",
    "\n",
    "**Best for**: Multiple filter dimensions\n",
    "**Query example**: `WHERE year=2024 AND region='US'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning examples in PySpark\n",
    "\n",
    "partitioning_code = \"\"\"\n",
    "# Write data with date partitioning\n",
    "df.write.mode(\"append\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .save(\"/mnt/data/transactions\")\n",
    "\n",
    "# Multiple partition columns\n",
    "df.write.mode(\"append\") \\\n",
    "    .partitionBy(\"year\", \"month\", \"region\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .save(\"/mnt/data/sales\")\n",
    "\n",
    "# Query leveraging partitions\n",
    "df = spark.read.format(\"delta\") \\\n",
    "    .load(\"/mnt/data/transactions\") \\\n",
    "    .filter(\"date >= '2024-01-01' AND date < '2024-02-01'\")\n",
    "# Only reads January 2024 partition!\n",
    "\"\"\"\n",
    "\n",
    "print(partitioning_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Best Practices\n",
    "\n",
    "✅ **DO**:\n",
    "- Partition by commonly filtered columns\n",
    "- Use date for time-series data\n",
    "- Keep partition size 100MB-1GB\n",
    "- Limit partition columns (1-3 usually)\n",
    "\n",
    "❌ **DON'T**:\n",
    "- Over-partition (too many small files)\n",
    "- Partition by high-cardinality columns (user_id)\n",
    "- Use too many partition levels (>3)\n",
    "- Partition small datasets (<1GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Slowly Changing Dimensions (SCD)\n",
    "\n",
    "Handling changes in dimensional data over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCD Type 1: Overwrite\n",
    "\n",
    "**Strategy**: Overwrite old values, no history\n",
    "\n",
    "**Use Case**: Corrections, unimportant changes\n",
    "\n",
    "**Example**: Fix customer email typo\n",
    "\n",
    "```\n",
    "Before: customer_id=1, email='old@email.com'\n",
    "After:  customer_id=1, email='new@email.com'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCD Type 1 implementation\n",
    "\n",
    "scd_type1 = \"\"\"\n",
    "# Simple UPDATE operation\n",
    "MERGE INTO customers target\n",
    "USING updates source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.email = source.email,\n",
    "             target.phone = source.phone,\n",
    "             target.updated_at = current_timestamp()\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *;\n",
    "\"\"\"\n",
    "\n",
    "print(scd_type1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCD Type 2: Track History\n",
    "\n",
    "**Strategy**: Keep full history with effective dates\n",
    "\n",
    "**Use Case**: Track changes over time (price history, address changes)\n",
    "\n",
    "**Implementation**: Add surrogate key, effective dates, current flag\n",
    "\n",
    "```\n",
    "Table: customer_history\n",
    "| surrogate_key | customer_id | address | effective_from | effective_to | is_current |\n",
    "|---------------|-------------|---------|----------------|--------------|------------|\n",
    "| 1 | 101 | \"123 Old St\" | 2023-01-01 | 2024-03-01 | False |\n",
    "| 2 | 101 | \"456 New Ave\" | 2024-03-01 | 9999-12-31 | True |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCD Type 2 implementation\n",
    "\n",
    "scd_type2 = \"\"\"\n",
    "# Close current record and insert new one\n",
    "\n",
    "# Step 1: Close expired records\n",
    "UPDATE customer_history\n",
    "SET effective_to = '2024-03-01',\n",
    "    is_current = False\n",
    "WHERE customer_id = 101\n",
    "  AND is_current = True;\n",
    "\n",
    "# Step 2: Insert new record\n",
    "INSERT INTO customer_history VALUES (\n",
    "  2,  -- new surrogate key\n",
    "  101,  -- same customer_id\n",
    "  '456 New Ave',  -- new address\n",
    "  '2024-03-01',  -- effective_from\n",
    "  '9999-12-31',  -- effective_to (future)\n",
    "  True  -- is_current\n",
    ");\n",
    "\n",
    "# Query current state\n",
    "SELECT * FROM customer_history WHERE is_current = True;\n",
    "\n",
    "# Query historical state\n",
    "SELECT * FROM customer_history \n",
    "WHERE customer_id = 101\n",
    "  AND '2023-06-15' BETWEEN effective_from AND effective_to;\n",
    "\"\"\"\n",
    "\n",
    "print(scd_type2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCD Type 3: Track Limited History\n",
    "\n",
    "**Strategy**: Keep previous value in separate column\n",
    "\n",
    "**Use Case**: Track one previous value only\n",
    "\n",
    "```\n",
    "| customer_id | current_address | previous_address | change_date |\n",
    "|-------------|----------------|------------------|-------------|\n",
    "| 101 | \"456 New Ave\" | \"123 Old St\" | 2024-03-01 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Medallion Architecture (Bronze-Silver-Gold)\n",
    "\n",
    "Databricks recommended pattern for organizing data lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Source Systems → BRONZE → SILVER → GOLD → Analytics/ML\n",
    "                 (Raw)   (Cleaned) (Aggregated)\n",
    "```\n",
    "\n",
    "### Bronze Layer (Raw)\n",
    "\n",
    "**Purpose**: Store raw, unprocessed data\n",
    "\n",
    "**Characteristics**:\n",
    "- Exact copy of source data\n",
    "- All data types (structured, semi-structured, unstructured)\n",
    "- Append-only (immutable history)\n",
    "- Includes ingestion metadata\n",
    "- No business logic applied\n",
    "\n",
    "**Example Structure**:\n",
    "```\n",
    "/bronze/\n",
    "  /orders_raw/\n",
    "  /customers_raw/\n",
    "  /clickstream_raw/\n",
    "```\n",
    "\n",
    "**Schema**:\n",
    "```python\n",
    "- source_file (string)\n",
    "- ingestion_timestamp (timestamp)\n",
    "- raw_data (string/binary)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze layer example\n",
    "\n",
    "bronze_code = \"\"\"\n",
    "# Ingest raw data to bronze\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .load(\"/source/orders/*.json\")\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "# Write to bronze (append-only)\n",
    "bronze_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"ingestion_date\") \\\n",
    "    .save(\"/bronze/orders_raw\")\n",
    "\"\"\"\n",
    "\n",
    "print(bronze_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silver Layer (Cleaned/Conformed)\n",
    "\n",
    "**Purpose**: Cleaned, validated, enriched data\n",
    "\n",
    "**Characteristics**:\n",
    "- Data quality checks applied\n",
    "- Standardized formats\n",
    "- Deduplication\n",
    "- Schema enforcement\n",
    "- Joins with dimension tables\n",
    "- Still detailed/granular\n",
    "\n",
    "**Transformations**:\n",
    "- Remove duplicates\n",
    "- Fix data types\n",
    "- Handle nulls\n",
    "- Standardize formats (dates, phone numbers)\n",
    "- Add business keys\n",
    "- Apply data quality rules\n",
    "\n",
    "**Example Structure**:\n",
    "```\n",
    "/silver/\n",
    "  /orders/\n",
    "  /customers/\n",
    "  /products/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silver layer example\n",
    "\n",
    "silver_code = \"\"\"\n",
    "# Transform bronze to silver\n",
    "from pyspark.sql.functions import col, to_date, when, trim, upper\n",
    "\n",
    "# Read from bronze\n",
    "bronze_df = spark.read.format(\"delta\").load(\"/bronze/orders_raw\")\n",
    "\n",
    "# Clean and transform\n",
    "silver_df = (\n",
    "    bronze_df\n",
    "    # Remove duplicates\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    \n",
    "    # Fix data types\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"decimal(10,2)\"))\n",
    "    \n",
    "    # Data quality: filter invalid records\n",
    "    .filter(col(\"order_id\").isNotNull())\n",
    "    .filter(col(\"amount\") > 0)\n",
    "    \n",
    "    # Standardize\n",
    "    .withColumn(\"country_code\", upper(trim(col(\"country_code\"))))\n",
    "    \n",
    "    # Add derived columns\n",
    "    .withColumn(\"year\", year(col(\"order_date\")))\n",
    "    .withColumn(\"month\", month(col(\"order_date\")))\n",
    "    \n",
    "    # Select final columns\n",
    "    .select(\n",
    "        \"order_id\", \"customer_id\", \"product_id\",\n",
    "        \"order_date\", \"amount\", \"country_code\",\n",
    "        \"year\", \"month\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Write to silver\n",
    "silver_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .save(\"/silver/orders\")\n",
    "\"\"\"\n",
    "\n",
    "print(silver_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gold Layer (Business-Level Aggregates)\n",
    "\n",
    "**Purpose**: Business-ready aggregated data\n",
    "\n",
    "**Characteristics**:\n",
    "- Aggregated metrics\n",
    "- Business logic applied\n",
    "- Optimized for reporting/analytics\n",
    "- Denormalized for performance\n",
    "- Feature tables for ML\n",
    "\n",
    "**Examples**:\n",
    "- Daily/Monthly sales summaries\n",
    "- Customer lifetime value\n",
    "- Product analytics\n",
    "- KPI dashboards\n",
    "- ML feature stores\n",
    "\n",
    "**Example Structure**:\n",
    "```\n",
    "/gold/\n",
    "  /daily_sales_by_region/\n",
    "  /customer_ltv/\n",
    "  /product_performance/\n",
    "  /ml_features/customer_features/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold layer example\n",
    "\n",
    "gold_code = \"\"\"\n",
    "# Aggregate silver to gold\n",
    "from pyspark.sql.functions import sum, count, avg, max, min\n",
    "\n",
    "# Read from silver\n",
    "orders_df = spark.read.format(\"delta\").load(\"/silver/orders\")\n",
    "customers_df = spark.read.format(\"delta\").load(\"/silver/customers\")\n",
    "\n",
    "# Create business-level aggregate: Daily sales by region\n",
    "daily_sales = (\n",
    "    orders_df\n",
    "    .join(customers_df, \"customer_id\")\n",
    "    .groupBy(\"order_date\", \"region\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        sum(\"amount\").alias(\"total_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\"),\n",
    "        count(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"revenue_per_customer\", \n",
    "                col(\"total_revenue\") / col(\"unique_customers\"))\n",
    ")\n",
    "\n",
    "# Write to gold\n",
    "daily_sales.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .save(\"/gold/daily_sales_by_region\")\n",
    "\n",
    "# ML Feature table\n",
    "customer_features = (\n",
    "    orders_df\n",
    "    .groupBy(\"customer_id\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        sum(\"amount\").alias(\"total_spent\"),\n",
    "        avg(\"amount\").alias(\"avg_order_value\"),\n",
    "        max(\"order_date\").alias(\"last_order_date\"),\n",
    "        min(\"order_date\").alias(\"first_order_date\")\n",
    "    )\n",
    "    .withColumn(\"customer_lifetime_days\",\n",
    "                datediff(col(\"last_order_date\"), col(\"first_order_date\")))\n",
    ")\n",
    "\n",
    "customer_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/gold/ml_features/customer_features\")\n",
    "\"\"\"\n",
    "\n",
    "print(gold_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medallion Architecture Benefits\n",
    "\n",
    "✅ **Clear Data Lineage**: Easy to trace data flow\n",
    "✅ **Incremental Processing**: Process only new data\n",
    "✅ **Data Quality**: Progressive refinement\n",
    "✅ **Flexibility**: Support multiple use cases\n",
    "✅ **Recovery**: Can rebuild downstream from upstream\n",
    "✅ **Governance**: Clear ownership and policies per layer\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Bronze**: Keep forever (immutable history)\n",
    "2. **Silver**: Apply quality checks, not business logic\n",
    "3. **Gold**: Optimize for specific use cases\n",
    "4. **Idempotency**: Re-running should produce same results\n",
    "5. **Documentation**: Document transformations at each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Batch vs Streaming processing paradigms\n",
    "✅ Evolution from Data Warehouse → Data Lake → Lakehouse\n",
    "✅ Partitioning strategies for performance\n",
    "✅ Slowly Changing Dimensions (SCD Types 1, 2, 3)\n",
    "✅ Medallion Architecture (Bronze-Silver-Gold)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review the concepts and examples\n",
    "2. Think about how to apply these to your use cases\n",
    "3. Move to [04-Environment-Setup.ipynb](./04-Environment-Setup.ipynb)\n",
    "4. Then proceed to Module 2 for hands-on Databricks\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Databricks Lakehouse Platform](https://www.databricks.com/product/data-lakehouse)\n",
    "- [Medallion Architecture Guide](https://www.databricks.com/glossary/medallion-architecture)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
