{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Fundamentals\n",
    "\n",
    "## Overview\n",
    "Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. This notebook covers Delta Lake fundamentals.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what Delta Lake is and why it matters\n",
    "- Create and manage Delta tables\n",
    "- Perform CRUD operations\n",
    "- Use ACID transactions\n",
    "- Leverage Delta Lake features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Delta Lake?\n",
    "\n",
    "### The Problem with Traditional Data Lakes\n",
    "\n",
    "Traditional data lakes using Parquet, CSV, or JSON have limitations:\n",
    "\n",
    "‚ùå **No ACID transactions**\n",
    "- Failed writes leave partial data\n",
    "- No atomicity for multi-file operations\n",
    "\n",
    "‚ùå **No schema enforcement**\n",
    "- Data quality issues\n",
    "- Schema mismatches\n",
    "\n",
    "‚ùå **Difficult updates and deletes**\n",
    "- Must rewrite entire partitions\n",
    "- No efficient row-level operations\n",
    "\n",
    "‚ùå **No time travel**\n",
    "- Can't query historical versions\n",
    "- Difficult to rollback changes\n",
    "\n",
    "‚ùå **Performance challenges**\n",
    "- Small file problem\n",
    "- No Z-ordering or statistics\n",
    "\n",
    "### Delta Lake Solution\n",
    "\n",
    "Delta Lake adds a transaction log (metadata layer) on top of cloud storage:\n",
    "\n",
    "```\n",
    "Delta Table = Parquet Files + Transaction Log\n",
    "```\n",
    "\n",
    "‚úÖ **ACID Transactions**: Atomicity, Consistency, Isolation, Durability\n",
    "‚úÖ **Schema Enforcement & Evolution**: Safe schema changes\n",
    "‚úÖ **Time Travel**: Query any historical version\n",
    "‚úÖ **Efficient Upserts**: MERGE operations\n",
    "‚úÖ **Performance**: OPTIMIZE, Z-ORDER, statistics\n",
    "‚úÖ **Unified Batch & Streaming**: Single API for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Delta Tables\n",
    "\n",
    "There are multiple ways to create Delta tables in Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using PySpark DataFrame Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Sample customer data\n",
    "data = [\n",
    "    (1, \"Alice\", \"alice@email.com\", \"2024-01-15\", \"NY\"),\n",
    "    (2, \"Bob\", \"bob@email.com\", \"2024-01-20\", \"CA\"),\n",
    "    (3, \"Charlie\", \"charlie@email.com\", \"2024-02-01\", \"TX\"),\n",
    "    (4, \"Diana\", \"diana@email.com\", \"2024-02-10\", \"NY\"),\n",
    "    (5, \"Eve\", \"eve@email.com\", \"2024-02-15\", \"FL\")\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"name\", \"email\", \"signup_date\", \"state\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert signup_date to date type\n",
    "df = df.withColumn(\"signup_date\", to_date(col(\"signup_date\")))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write as Delta table\n",
    "delta_path = \"/tmp/delta/customers\"\n",
    "\n",
    "df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"Delta table created at: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Delta table\n",
    "customers_df = spark.read.format(\"delta\").load(delta_path)\n",
    "display(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using SQL CREATE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Delta table using SQL\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS orders (\n",
    "    order_id BIGINT,\n",
    "    customer_id BIGINT,\n",
    "    product_id STRING,\n",
    "    amount DECIMAL(10, 2),\n",
    "    order_date DATE,\n",
    "    status STRING\n",
    ")\n",
    "USING DELTA\n",
    "PARTITIONED BY (order_date)\n",
    "LOCATION '/tmp/delta/orders'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Orders table created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data using SQL\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO orders VALUES\n",
    "    (101, 1, 'PROD-001', 150.00, '2024-02-01', 'completed'),\n",
    "    (102, 1, 'PROD-002', 200.50, '2024-02-02', 'completed'),\n",
    "    (103, 2, 'PROD-001', 75.25, '2024-02-03', 'pending'),\n",
    "    (104, 3, 'PROD-003', 300.00, '2024-02-04', 'completed'),\n",
    "    (105, 4, 'PROD-002', 120.75, '2024-02-05', 'shipped')\n",
    "\"\"\")\n",
    "\n",
    "# Query the table\n",
    "display(spark.sql(\"SELECT * FROM orders ORDER BY order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: SaveAsTable (Managed Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create managed Delta table\n",
    "# Data is stored in default warehouse location\n",
    "\n",
    "products_data = [\n",
    "    (\"PROD-001\", \"Laptop\", \"Electronics\", 999.99),\n",
    "    (\"PROD-002\", \"Phone\", \"Electronics\", 599.99),\n",
    "    (\"PROD-003\", \"Desk\", \"Furniture\", 299.99),\n",
    "    (\"PROD-004\", \"Chair\", \"Furniture\", 149.99)\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(\n",
    "    products_data, \n",
    "    [\"product_id\", \"product_name\", \"category\", \"price\"]\n",
    ")\n",
    "\n",
    "products_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"products\")\n",
    "\n",
    "display(spark.table(\"products\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Delta Table Operations\n",
    "\n",
    "### Reading Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read using path\n",
    "df1 = spark.read.format(\"delta\").load(\"/tmp/delta/customers\")\n",
    "\n",
    "# Read using table name\n",
    "df2 = spark.table(\"orders\")\n",
    "\n",
    "# Read with SQL\n",
    "df3 = spark.sql(\"SELECT * FROM products WHERE category = 'Electronics'\")\n",
    "\n",
    "print(f\"Customers: {df1.count()} rows\")\n",
    "print(f\"Orders: {df2.count()} rows\")\n",
    "print(f\"Electronics: {df3.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update using SQL\n",
    "spark.sql(\"\"\"\n",
    "UPDATE orders\n",
    "SET status = 'completed'\n",
    "WHERE status = 'shipped'\n",
    "\"\"\")\n",
    "\n",
    "# Verify update\n",
    "display(spark.sql(\"SELECT * FROM orders WHERE order_id = 105\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update using Python Delta Table API\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta/customers\")\n",
    "\n",
    "delta_table.update(\n",
    "    condition = \"state = 'NY'\",\n",
    "    set = {\"state\": lit(\"NEW YORK\")}\n",
    ")\n",
    "\n",
    "# Verify\n",
    "display(spark.read.format(\"delta\").load(\"/tmp/delta/customers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete using SQL\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM orders\n",
    "WHERE status = 'pending' AND order_date < '2024-02-03'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Orders remaining: {spark.table('orders').count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete using Python API\n",
    "delta_table = DeltaTable.forName(spark, \"orders\")\n",
    "\n",
    "delta_table.delete(\"amount < 100\")\n",
    "\n",
    "print(f\"Orders remaining: {spark.table('orders').count()}\")\n",
    "display(spark.table(\"orders\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ACID Transactions\n",
    "\n",
    "Delta Lake guarantees ACID properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atomicity\n",
    "\n",
    "All or nothing - either entire operation succeeds or none of it does.\n",
    "\n",
    "```python\n",
    "# If this write fails mid-way, no partial data is visible\n",
    "large_df.write.format(\"delta\").save(\"/path/to/table\")\n",
    "```\n",
    "\n",
    "### Consistency\n",
    "\n",
    "Data remains in a valid state before and after transaction.\n",
    "\n",
    "```python\n",
    "# Schema enforcement prevents invalid data\n",
    "# This will fail if schema doesn't match\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/path/to/table\")\n",
    "```\n",
    "\n",
    "### Isolation\n",
    "\n",
    "Concurrent operations don't interfere with each other.\n",
    "\n",
    "```python\n",
    "# Multiple writers can write simultaneously\n",
    "# Readers always see consistent snapshot\n",
    "```\n",
    "\n",
    "### Durability\n",
    "\n",
    "Once committed, changes are permanent.\n",
    "\n",
    "```python\n",
    "# Transaction log ensures durability\n",
    "# Can recover from failures\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of concurrent writes (safe with Delta)\n",
    "from datetime import datetime\n",
    "\n",
    "# Writer 1: Add new customer\n",
    "new_customer = spark.createDataFrame(\n",
    "    [(6, \"Frank\", \"frank@email.com\", datetime.now().date(), \"CA\")],\n",
    "    [\"customer_id\", \"name\", \"email\", \"signup_date\", \"state\"]\n",
    ")\n",
    "\n",
    "new_customer.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/tmp/delta/customers\")\n",
    "\n",
    "print(\"Customer added successfully\")\n",
    "\n",
    "# Reader: Always sees consistent state\n",
    "print(f\"Total customers: {spark.read.format('delta').load('/tmp/delta/customers').count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Schema Enforcement and Evolution\n",
    "\n",
    "### Schema Enforcement\n",
    "\n",
    "Delta Lake prevents writes with incompatible schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current schema\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/customers\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to write with missing column - will fail\n",
    "try:\n",
    "    bad_data = spark.createDataFrame(\n",
    "        [(7, \"Grace\", \"grace@email.com\")],  # Missing columns!\n",
    "        [\"customer_id\", \"name\", \"email\"]\n",
    "    )\n",
    "    \n",
    "    bad_data.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/customers\")\n",
    "except Exception as e:\n",
    "    print(f\"Expected error: {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Evolution\n",
    "\n",
    "Allow controlled schema changes with `mergeSchema` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column with schema evolution\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "evolved_data = spark.createDataFrame(\n",
    "    [(7, \"Grace\", \"grace@email.com\", datetime.now().date(), \"WA\", \"555-0123\")],\n",
    "    [\"customer_id\", \"name\", \"email\", \"signup_date\", \"state\", \"phone\"]  # New column!\n",
    ")\n",
    "\n",
    "# Write with schema evolution enabled\n",
    "evolved_data.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"/tmp/delta/customers\")\n",
    "\n",
    "print(\"Schema evolved successfully!\")\n",
    "\n",
    "# Check new schema\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/customers\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data - old records have NULL for new column\n",
    "display(spark.read.format(\"delta\").load(\"/tmp/delta/customers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding the Transaction Log\n",
    "\n",
    "Delta Lake's magic is in the transaction log (`_delta_log` directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Delta table structure\n",
    "display(dbutils.fs.ls(\"/tmp/delta/customers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View transaction log\n",
    "display(dbutils.fs.ls(\"/tmp/delta/customers/_delta_log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe table history\n",
    "display(spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta/customers`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Partitioning Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partitioned Delta table\n",
    "sales_data = [\n",
    "    (1, \"2024-01-15\", \"US\", 1000.0),\n",
    "    (2, \"2024-01-16\", \"US\", 1500.0),\n",
    "    (3, \"2024-01-15\", \"EU\", 2000.0),\n",
    "    (4, \"2024-02-01\", \"US\", 1200.0),\n",
    "    (5, \"2024-02-01\", \"APAC\", 3000.0),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"sale_date\", \"region\", \"amount\"]\n",
    ").withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
    "\n",
    "# Write with partitioning\n",
    "sales_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .partitionBy(\"sale_date\", \"region\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/sales\")\n",
    "\n",
    "print(\"Partitioned table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View partition structure\n",
    "display(dbutils.fs.ls(\"/tmp/delta/sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query leveraging partitions (partition pruning)\n",
    "# This will only read US partition\n",
    "us_sales = spark.read.format(\"delta\").load(\"/tmp/delta/sales\") \\\n",
    "    .filter(\"region = 'US' AND sale_date >= '2024-01-15'\")\n",
    "\n",
    "display(us_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Create a Product Inventory Table\n",
    "Create a Delta table for product inventory with schema: product_id, product_name, quantity, last_updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Your code here\n",
    "\n",
    "# TODO: Create sample inventory data\n",
    "# TODO: Write as Delta table with partitioning by product category\n",
    "# TODO: Query the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Perform CRUD Operations\n",
    "On the inventory table:\n",
    "1. Insert 5 new products\n",
    "2. Update quantity for 2 products\n",
    "3. Delete 1 product\n",
    "4. Query to verify changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Your code here\n",
    "\n",
    "# TODO: INSERT new products\n",
    "# TODO: UPDATE quantities\n",
    "# TODO: DELETE a product\n",
    "# TODO: SELECT to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ What Delta Lake is and why it's important\n",
    "‚úÖ How to create Delta tables (multiple methods)\n",
    "‚úÖ CRUD operations (Create, Read, Update, Delete)\n",
    "‚úÖ ACID transaction guarantees\n",
    "‚úÖ Schema enforcement and evolution\n",
    "‚úÖ Transaction log architecture\n",
    "‚úÖ Partitioning Delta tables\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the practice exercises\n",
    "2. Experiment with your own data\n",
    "3. Move to [02-Delta-Lake-Advanced-Features.ipynb](./02-Delta-Lake-Advanced-Features.ipynb)\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "üí° **Delta Lake = Parquet + Transaction Log**\n",
    "üí° **ACID transactions make data reliable**\n",
    "üí° **Schema enforcement prevents bad data**\n",
    "üí° **Use Delta Lake for all production workloads**\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)\n",
    "- [Databricks Delta Guide](https://docs.databricks.com/delta/)\n",
    "- [Delta Lake GitHub](https://github.com/delta-io/delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
