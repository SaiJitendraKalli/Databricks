{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Questions - Set 1: Databricks Lakehouse Fundamentals\n",
    "\n",
    "## Overview\n",
    "This notebook contains 25 practice questions covering Databricks Lakehouse fundamentals, Delta Lake basics, and core concepts.\n",
    "\n",
    "## Instructions\n",
    "- Read each question carefully\n",
    "- Try to answer before revealing the solution\n",
    "- Review explanations for all questions, even correct ones\n",
    "- Track your score to identify weak areas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Databricks Architecture (Questions 1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "**Which of the following best describes the Databricks Lakehouse platform?**\n",
    "\n",
    "A) A data warehouse built on cloud storage  \n",
    "B) A data lake with ACID transactions and data management features  \n",
    "C) A replacement for Apache Spark  \n",
    "D) A NoSQL database optimized for analytics  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "The Databricks Lakehouse combines the best of data lakes and data warehouses. It provides:\n",
    "- Cost-effective storage of data lakes (on cloud object storage)\n",
    "- ACID transactions and data management features of data warehouses\n",
    "- Support for both structured and unstructured data\n",
    "- Unified platform for BI, ML, and streaming\n",
    "\n",
    "Delta Lake is the technology that enables these features on top of data lakes.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**What are the three levels in Unity Catalog's namespace hierarchy?**\n",
    "\n",
    "A) Database.Schema.Table  \n",
    "B) Catalog.Database.Table  \n",
    "C) Catalog.Schema.Table  \n",
    "D) Workspace.Database.Table  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: C**\n",
    "\n",
    "**Explanation:**\n",
    "Unity Catalog uses a three-level namespace:\n",
    "1. **Catalog** - Top level, represents a collection of schemas\n",
    "2. **Schema** (or Database) - Contains tables, views, and functions\n",
    "3. **Table** (or View) - The actual data objects\n",
    "\n",
    "Example: `my_catalog.sales_schema.customer_table`\n",
    "\n",
    "This structure provides better organization and governance compared to traditional two-level namespaces.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Which cluster type is recommended for production jobs that run on a schedule?**\n",
    "\n",
    "A) All-purpose cluster  \n",
    "B) Job cluster  \n",
    "C) SQL Warehouse  \n",
    "D) High-concurrency cluster  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "**Job clusters** are recommended for production scheduled jobs because:\n",
    "- Created when job starts, terminated when complete\n",
    "- More cost-effective (no idle time charges)\n",
    "- Isolated from other workloads\n",
    "- Optimized for specific job requirements\n",
    "\n",
    "**All-purpose clusters** are for:\n",
    "- Interactive analysis and development\n",
    "- Shared across users\n",
    "- More expensive for automated jobs\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "**What is the primary benefit of using Photon in Databricks?**\n",
    "\n",
    "A) Reduces storage costs  \n",
    "B) Provides better data governance  \n",
    "C) Accelerates query performance  \n",
    "D) Enables real-time streaming  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: C**\n",
    "\n",
    "**Explanation:**\n",
    "**Photon** is a vectorized query engine that:\n",
    "- Accelerates SQL and DataFrame queries\n",
    "- Written in C++ (vs standard Spark in JVM)\n",
    "- Provides 2-10x performance improvement\n",
    "- Especially effective for:\n",
    "  - Aggregate queries\n",
    "  - Joins\n",
    "  - File format operations\n",
    "\n",
    "Enabled automatically on SQL Warehouses and can be enabled on clusters.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "**In the medallion architecture, which layer contains raw, unprocessed data?**\n",
    "\n",
    "A) Bronze  \n",
    "B) Silver  \n",
    "C) Gold  \n",
    "D) Platinum  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: A**\n",
    "\n",
    "**Explanation:**\n",
    "The medallion architecture has three layers:\n",
    "\n",
    "1. **Bronze** (Raw):\n",
    "   - Raw data as ingested from source\n",
    "   - No transformations applied\n",
    "   - Append-only, immutable history\n",
    "\n",
    "2. **Silver** (Cleaned):\n",
    "   - Cleaned and validated data\n",
    "   - Deduplication, quality checks\n",
    "   - Enriched with additional context\n",
    "\n",
    "3. **Gold** (Curated):\n",
    "   - Business-level aggregates\n",
    "   - Optimized for analytics and reporting\n",
    "   - Feature tables for ML\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Delta Lake Basics (Questions 6-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "**What does ACID stand for in the context of Delta Lake?**\n",
    "\n",
    "A) Automated, Consistent, Isolated, Durable  \n",
    "B) Atomic, Consistent, Isolated, Durable  \n",
    "C) Atomic, Continuous, Isolated, Distributed  \n",
    "D) Automated, Continuous, Independent, Durable  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "ACID stands for:\n",
    "- **Atomicity**: Operations either complete fully or not at all\n",
    "- **Consistency**: Data remains in valid state before/after transaction\n",
    "- **Isolation**: Concurrent operations don't interfere\n",
    "- **Durability**: Committed changes are permanent\n",
    "\n",
    "Delta Lake provides ACID guarantees through its transaction log.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "**Which command is used to query a previous version of a Delta table?**\n",
    "\n",
    "A) `SELECT * FROM table@version`  \n",
    "B) `SELECT * FROM table VERSION AS OF 5`  \n",
    "C) `SELECT * FROM table HISTORY 5`  \n",
    "D) `SELECT * FROM table ROLLBACK TO 5`  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "Time travel in Delta Lake uses:\n",
    "\n",
    "```sql\n",
    "-- By version number\n",
    "SELECT * FROM table VERSION AS OF 5\n",
    "\n",
    "-- By timestamp\n",
    "SELECT * FROM table TIMESTAMP AS OF '2024-01-01'\n",
    "```\n",
    "\n",
    "Or in Python:\n",
    "```python\n",
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/path\")\n",
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2024-01-01\").load(\"/path\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "**What is the purpose of the OPTIMIZE command in Delta Lake?**\n",
    "\n",
    "A) To delete old data  \n",
    "B) To compact small files into larger ones  \n",
    "C) To add new partitions  \n",
    "D) To update table statistics only  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "`OPTIMIZE` compacts small files into larger files:\n",
    "\n",
    "```sql\n",
    "OPTIMIZE table_name;\n",
    "\n",
    "-- With Z-ORDER for specific columns\n",
    "OPTIMIZE table_name ZORDER BY (column1, column2);\n",
    "```\n",
    "\n",
    "Benefits:\n",
    "- Improves query performance (fewer files to read)\n",
    "- Reduces metadata overhead\n",
    "- Optimal file size is 32MB-1GB\n",
    "\n",
    "Note: VACUUM is used to delete old files, not OPTIMIZE.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "**Which statement about Delta Lake schema enforcement is TRUE?**\n",
    "\n",
    "A) Schema changes are always allowed automatically  \n",
    "B) Schema enforcement prevents writes with incompatible schemas  \n",
    "C) Delta Lake does not support schema enforcement  \n",
    "D) Schema enforcement only applies to Parquet files  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "Delta Lake enforces schemas by default:\n",
    "- Writes with incompatible schemas are rejected\n",
    "- Prevents data quality issues\n",
    "- Ensures consistency\n",
    "\n",
    "To evolve schema (add columns):\n",
    "```python\n",
    "df.write.format(\"delta\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"/path\")\n",
    "```\n",
    "\n",
    "Or allow schema evolution:\n",
    "```sql\n",
    "SET spark.databricks.delta.schema.autoMerge.enabled = true;\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "**What is the minimum retention period for VACUUM command by default?**\n",
    "\n",
    "A) 0 days  \n",
    "B) 7 days  \n",
    "C) 30 days  \n",
    "D) 90 days  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "The default retention period is **7 days** (168 hours):\n",
    "\n",
    "```sql\n",
    "-- Default: keeps files from last 7 days\n",
    "VACUUM table_name;\n",
    "\n",
    "-- Custom retention (30 days)\n",
    "VACUUM table_name RETAIN 720 HOURS;\n",
    "```\n",
    "\n",
    "This protects:\n",
    "- Time travel queries\n",
    "- Long-running queries\n",
    "- Concurrent operations\n",
    "\n",
    "‚ö†Ô∏è **Warning**: Setting retention < 7 days requires:\n",
    "```sql\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: PySpark and SQL (Questions 11-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "**What is the difference between transformations and actions in Spark?**\n",
    "\n",
    "A) Transformations execute immediately, actions are lazy  \n",
    "B) Transformations are lazy, actions trigger execution  \n",
    "C) There is no difference  \n",
    "D) Transformations modify data, actions read data  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Transformations (Lazy)**:\n",
    "- Create new DataFrames\n",
    "- Not executed immediately\n",
    "- Examples: `select()`, `filter()`, `groupBy()`, `join()`\n",
    "\n",
    "**Actions (Eager)**:\n",
    "- Trigger computation\n",
    "- Return results\n",
    "- Examples: `show()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "```python\n",
    "# Lazy - not executed\n",
    "df2 = df.filter(col(\"age\") > 21)\n",
    "\n",
    "# Action - triggers execution\n",
    "df2.show()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "**Which SQL function is used to extract year from a date column?**\n",
    "\n",
    "A) `EXTRACT(YEAR FROM date_column)`  \n",
    "B) `YEAR(date_column)`  \n",
    "C) `DATE_PART('year', date_column)`  \n",
    "D) All of the above  \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "**Answer: B** (most common in Databricks SQL)\n",
    "\n",
    "**Explanation:**\n",
    "In Databricks/Spark SQL:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  YEAR(order_date) as year,\n",
    "  MONTH(order_date) as month,\n",
    "  DAY(order_date) as day\n",
    "FROM orders;\n",
    "```\n",
    "\n",
    "In PySpark:\n",
    "```python\n",
    "from pyspark.sql.functions import year, month, day\n",
    "\n",
    "df.select(\n",
    "  year(col(\"order_date\")).alias(\"year\"),\n",
    "  month(col(\"order_date\")).alias(\"month\")\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Score Tracker\n",
    "\n",
    "Track your score as you go through the questions:\n",
    "\n",
    "- Questions 1-5 (Architecture): __ / 5\n",
    "- Questions 6-10 (Delta Lake): __ / 5\n",
    "- Questions 11-15 (PySpark/SQL): __ / 5\n",
    "\n",
    "**Total: __ / 15**\n",
    "\n",
    "### Score Interpretation\n",
    "- 13-15: Excellent! You're well-prepared\n",
    "- 10-12: Good! Review missed topics\n",
    "- 7-9: Fair. Focus study on weak areas\n",
    "- < 7: Need more study. Review all materials\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This practice set covered:\n",
    "- ‚úÖ Databricks architecture and components\n",
    "- ‚úÖ Unity Catalog fundamentals\n",
    "- ‚úÖ Delta Lake ACID properties and operations\n",
    "- ‚úÖ Medallion architecture\n",
    "- ‚úÖ PySpark transformations and actions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Review any questions you missed\n",
    "2. Study the explanations thoroughly\n",
    "3. Practice with hands-on exercises\n",
    "4. Move to Practice Set 2 for more questions\n",
    "\n",
    "## Study Resources\n",
    "\n",
    "- [Databricks Documentation](https://docs.databricks.com/)\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/)\n",
    "\n",
    "**Good luck with your certification! üéì**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
