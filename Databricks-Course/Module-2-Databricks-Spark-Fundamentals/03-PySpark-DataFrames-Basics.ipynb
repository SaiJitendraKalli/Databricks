{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames Basics\n",
    "\n",
    "## Overview\n",
    "This notebook introduces PySpark DataFrames - the fundamental data structure in Spark for structured data processing.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand DataFrames and their benefits\n",
    "- Create DataFrames from various sources\n",
    "- Perform basic transformations\n",
    "- Understand lazy evaluation and actions\n",
    "- Work with schemas and data types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are DataFrames?\n",
    "\n",
    "### DataFrame Concept\n",
    "\n",
    "A **DataFrame** is a distributed collection of data organized into named columns, similar to a table in a relational database or a pandas DataFrame, but with optimizations for distributed computing.\n",
    "\n",
    "**Key Features**:\n",
    "- ✅ Distributed across cluster nodes\n",
    "- ✅ Immutable (transformations create new DataFrames)\n",
    "- ✅ Lazy evaluation (operations not executed until action)\n",
    "- ✅ Optimized execution plans (Catalyst optimizer)\n",
    "- ✅ Type-safe with schema\n",
    "\n",
    "**DataFrame vs RDD**:\n",
    "- DataFrames have named columns and schema\n",
    "- Better optimization and performance\n",
    "- Easier to use (SQL-like operations)\n",
    "- DataFrames are recommended over RDDs for most use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "### Method 1: From Python Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# In Databricks, spark session is already available\n",
    "# For local development:\n",
    "# spark = SparkSession.builder.appName(\"DataFrameBasics\").getOrCreate()\n",
    "\n",
    "# Create DataFrame from list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\"),\n",
    "    (2, \"Bob\", 30, \"Sales\"),\n",
    "    (3, \"Charlie\", 35, \"Engineering\"),\n",
    "    (4, \"Diana\", 28, \"Marketing\"),\n",
    "    (5, \"Eve\", 32, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display DataFrame (Databricks command)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame (standard Spark command)\n",
    "df.show()\n",
    "\n",
    "# Show with options\n",
    "df.show(3, truncate=False)  # Show 3 rows, don't truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From Files (CSV, JSON, Parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "# df_csv = spark.read.csv(\"/path/to/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read with schema specification\n",
    "# df_csv = spark.read \\\n",
    "#     .option(\"header\", \"true\") \\\n",
    "#     .option(\"inferSchema\", \"true\") \\\n",
    "#     .csv(\"/path/to/file.csv\")\n",
    "\n",
    "# Read JSON\n",
    "# df_json = spark.read.json(\"/path/to/file.json\")\n",
    "\n",
    "# Read Parquet (columnar format)\n",
    "# df_parquet = spark.read.parquet(\"/path/to/file.parquet\")\n",
    "\n",
    "# Read Delta (recommended in Databricks)\n",
    "# df_delta = spark.read.format(\"delta\").load(\"/path/to/delta/table\")\n",
    "\n",
    "print(\"File reading examples (commented out - need actual files)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: From SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Create DataFrame from SQL query\n",
    "df_sql = spark.sql(\"\"\"\n",
    "    SELECT id, name, age, department\n",
    "    FROM employees\n",
    "    WHERE age > 28\n",
    "\"\"\")\n",
    "\n",
    "display(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame Schema\n",
    "\n",
    "Schema defines the structure of your DataFrame - column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()\n",
    "\n",
    "# Get schema\n",
    "print(\"\\nSchema object:\")\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "data_with_salary = [\n",
    "    (1, \"Alice\", 25, \"Engineering\", 75000.0),\n",
    "    (2, \"Bob\", 30, \"Sales\", 65000.0),\n",
    "    (3, \"Charlie\", 35, \"Engineering\", 85000.0),\n",
    "]\n",
    "\n",
    "df_typed = spark.createDataFrame(data_with_salary, schema)\n",
    "df_typed.printSchema()\n",
    "display(df_typed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Select using col() function\n",
    "df.select(col(\"name\"), col(\"age\")).show()\n",
    "\n",
    "# Select all columns\n",
    "df.select(\"*\").show()\n",
    "\n",
    "# Select with expressions\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    (col(\"age\") + 5).alias(\"age_in_5_years\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with condition\n",
    "df.filter(col(\"age\") > 28).show()\n",
    "\n",
    "# Alternative: where() is alias for filter()\n",
    "df.where(col(\"age\") > 28).show()\n",
    "\n",
    "# Multiple conditions with &, |, ~\n",
    "df.filter(\n",
    "    (col(\"age\") > 25) & (col(\"department\") == \"Engineering\")\n",
    ").show()\n",
    "\n",
    "# Filter with SQL expression\n",
    "df.filter(\"age > 28 AND department = 'Engineering'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column with withColumn()\n",
    "df_with_bonus = df.withColumn(\n",
    "    \"bonus\",\n",
    "    when(col(\"department\") == \"Engineering\", 5000)\n",
    "    .when(col(\"department\") == \"Sales\", 3000)\n",
    "    .otherwise(2000)\n",
    ")\n",
    "\n",
    "display(df_with_bonus)\n",
    "\n",
    "# Add multiple columns\n",
    "df_enhanced = df \\\n",
    "    .withColumn(\"age_category\", \n",
    "                when(col(\"age\") < 30, \"Young\")\n",
    "                .otherwise(\"Senior\")) \\\n",
    "    .withColumn(\"name_length\", length(col(\"name\")))\n",
    "\n",
    "display(df_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename single column\n",
    "df.withColumnRenamed(\"name\", \"employee_name\").show()\n",
    "\n",
    "# Rename multiple columns (using select with alias)\n",
    "df.select(\n",
    "    col(\"id\").alias(\"employee_id\"),\n",
    "    col(\"name\").alias(\"employee_name\"),\n",
    "    col(\"age\"),\n",
    "    col(\"department\").alias(\"dept\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop single column\n",
    "df.drop(\"age\").show()\n",
    "\n",
    "# Drop multiple columns\n",
    "df.drop(\"age\", \"department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregations and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aggregations\n",
    "print(\"Total count:\", df.count())\n",
    "print(\"Average age:\", df.select(avg(\"age\")).collect()[0][0])\n",
    "\n",
    "# Multiple aggregations\n",
    "df.select(\n",
    "    count(\"*\").alias(\"total_count\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    min(\"age\").alias(\"min_age\"),\n",
    "    max(\"age\").alias(\"max_age\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by department\n",
    "df.groupBy(\"department\").count().show()\n",
    "\n",
    "# Group by with aggregations\n",
    "df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    min(\"age\").alias(\"youngest\"),\n",
    "    max(\"age\").alias(\"oldest\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by single column\n",
    "df.orderBy(\"age\").show()\n",
    "\n",
    "# Sort descending\n",
    "df.orderBy(col(\"age\").desc()).show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "df.orderBy(col(\"department\").asc(), col(\"age\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with nulls\n",
    "data_with_nulls = [\n",
    "    (1, \"Alice\", 25, \"Engineering\"),\n",
    "    (2, \"Bob\", None, \"Sales\"),\n",
    "    (3, None, 35, \"Engineering\"),\n",
    "    (4, \"Diana\", 28, None),\n",
    "]\n",
    "\n",
    "df_nulls = spark.createDataFrame(data_with_nulls, [\"id\", \"name\", \"age\", \"department\"])\n",
    "display(df_nulls)\n",
    "\n",
    "# Drop rows with any null\n",
    "print(\"Drop any nulls:\")\n",
    "df_nulls.dropna().show()\n",
    "\n",
    "# Drop rows where all values are null\n",
    "print(\"Drop all nulls:\")\n",
    "df_nulls.dropna(how=\"all\").show()\n",
    "\n",
    "# Drop based on specific columns\n",
    "print(\"Drop nulls in specific columns:\")\n",
    "df_nulls.dropna(subset=[\"name\", \"age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values\n",
    "print(\"Fill all nulls with default:\")\n",
    "df_nulls.fillna(\"Unknown\").show()\n",
    "\n",
    "# Fill with different values per column\n",
    "print(\"Fill with column-specific values:\")\n",
    "df_nulls.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"age\": 0,\n",
    "    \"department\": \"Unassigned\"\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transformations vs Actions\n",
    "\n",
    "### Understanding Lazy Evaluation\n",
    "\n",
    "**Transformations** (lazy - not executed immediately):\n",
    "- `select()`, `filter()`, `where()`, `groupBy()`\n",
    "- `withColumn()`, `drop()`, `orderBy()`\n",
    "- Return a new DataFrame\n",
    "\n",
    "**Actions** (eager - trigger execution):\n",
    "- `show()`, `count()`, `collect()`\n",
    "- `write()`, `save()`\n",
    "- Return results to driver or write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations are lazy - these don't execute yet\n",
    "df_transformed = df \\\n",
    "    .filter(col(\"age\") > 25) \\\n",
    "    .withColumn(\"age_next_year\", col(\"age\") + 1) \\\n",
    "    .select(\"name\", \"age\", \"age_next_year\")\n",
    "\n",
    "print(\"Transformations defined but not executed yet\")\n",
    "\n",
    "# Action triggers execution\n",
    "print(\"\\nNow executing with show():\")\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Common DataFrame Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Get row count\n",
    "print(\"Row count:\", df.count())\n",
    "\n",
    "# Get number of partitions\n",
    "print(\"Partitions:\", df.rdd.getNumPartitions())\n",
    "\n",
    "# Describe statistics\n",
    "df.describe().show()\n",
    "\n",
    "# Get distinct values\n",
    "print(\"\\nDistinct departments:\")\n",
    "df.select(\"department\").distinct().show()\n",
    "\n",
    "# Get first n rows\n",
    "print(\"\\nFirst 2 rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "# Take n rows\n",
    "print(\"\\nTake 2 rows:\")\n",
    "print(df.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for exercises\n",
    "sales_data = [\n",
    "    (1, \"Product A\", 100, 50, \"2024-01-15\", \"North\"),\n",
    "    (2, \"Product B\", 150, 30, \"2024-01-16\", \"South\"),\n",
    "    (3, \"Product A\", 100, 75, \"2024-01-17\", \"North\"),\n",
    "    (4, \"Product C\", 200, 25, \"2024-01-18\", \"East\"),\n",
    "    (5, \"Product B\", 150, 40, \"2024-01-19\", \"West\"),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"id\", \"product\", \"price\", \"quantity\", \"date\", \"region\"]\n",
    ")\n",
    "\n",
    "display(sales_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate total revenue per product\n",
    "Add a column 'revenue' = price * quantity, then group by product and sum revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Add revenue column and calculate total per product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Find products with average quantity > 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Group by product, calculate avg quantity, filter > 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ What DataFrames are and their benefits\n",
    "✅ Creating DataFrames from various sources\n",
    "✅ Working with schemas and data types\n",
    "✅ Basic transformations (select, filter, withColumn)\n",
    "✅ Aggregations and grouping\n",
    "✅ Handling missing data\n",
    "✅ Transformations vs Actions (lazy evaluation)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the practice exercises\n",
    "2. Experiment with your own data\n",
    "3. Move to advanced DataFrame operations\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "- [Databricks PySpark Guide](https://docs.databricks.com/pyspark/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
