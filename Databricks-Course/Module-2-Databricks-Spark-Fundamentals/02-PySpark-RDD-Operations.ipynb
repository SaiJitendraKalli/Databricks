{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark RDD Operations\n",
    "\n",
    "## Overview\n",
    "This notebook covers Resilient Distributed Datasets (RDDs) - the fundamental data structure in Spark, along with transformations and actions.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RDD concepts and when to use them\n",
    "- Create RDDs from various sources\n",
    "- Apply transformations (map, filter, flatMap, etc.)\n",
    "- Execute actions (collect, reduce, count, etc.)\n",
    "- Work with pair RDDs\n",
    "- Understand partitioning and persistence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD Basics\n",
    "\n",
    "### What is an RDD?\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** is:\n",
    "- An immutable distributed collection of objects\n",
    "- Partitioned across the cluster\n",
    "- Fault-tolerant (can be reconstructed if lost)\n",
    "- Low-level API (DataFrames are built on top of RDDs)\n",
    "\n",
    "### When to Use RDDs vs DataFrames?\n",
    "\n",
    "**Use RDDs when:**\n",
    "- Need fine-grained control over data\n",
    "- Working with unstructured data\n",
    "- Need to manipulate data at low level\n",
    "- Legacy code requires RDDs\n",
    "\n",
    "**Use DataFrames when:**\n",
    "- Working with structured/semi-structured data\n",
    "- Want automatic optimization\n",
    "- Need SQL-like operations\n",
    "- Better performance in most cases (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# In Databricks, sc (SparkContext) is already available\n",
    "# For local: sc = SparkContext(\"local\", \"RDD Operations\")\n",
    "\n",
    "# Method 1: From Python collection\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "print(f\"RDD created with {rdd.count()} elements\")\n",
    "print(f\"First 5 elements: {rdd.take(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From text file\n",
    "# text_rdd = sc.textFile(\"/path/to/file.txt\")\n",
    "\n",
    "# Method 3: From DataFrame\n",
    "df = spark.range(1, 11)\n",
    "rdd_from_df = df.rdd\n",
    "\n",
    "print(f\"RDD from DataFrame: {rdd_from_df.take(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD with specific number of partitions\n",
    "rdd_4_partitions = sc.parallelize(range(1, 101), 4)\n",
    "\n",
    "print(f\"Number of partitions: {rdd_4_partitions.getNumPartitions()}\")\n",
    "print(f\"Elements per partition: {rdd_4_partitions.glom().map(len).collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations (Lazy Operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map() - Transform each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square each number\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
    "squared = numbers.map(lambda x: x ** 2)\n",
    "\n",
    "print(f\"Original: {numbers.collect()}\")\n",
    "print(f\"Squared: {squared.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter() - Keep elements matching condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only even numbers\n",
    "evens = numbers.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(f\"Original: {numbers.collect()}\")\n",
    "print(f\"Evens: {evens.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() - Transform and flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentences into words\n",
    "sentences = sc.parallelize([\n",
    "    \"Hello world\",\n",
    "    \"PySpark tutorial\",\n",
    "    \"Databricks course\"\n",
    "])\n",
    "\n",
    "# map returns list of lists\n",
    "words_map = sentences.map(lambda x: x.split())\n",
    "print(f\"Using map: {words_map.collect()}\")\n",
    "\n",
    "# flatMap flattens the result\n",
    "words_flatmap = sentences.flatMap(lambda x: x.split())\n",
    "print(f\"Using flatMap: {words_flatmap.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct() - Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "duplicates = sc.parallelize([1, 2, 2, 3, 3, 3, 4, 5, 5])\n",
    "unique = duplicates.distinct()\n",
    "\n",
    "print(f\"With duplicates: {duplicates.collect()}\")\n",
    "print(f\"Distinct: {unique.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union() - Combine RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union of two RDDs\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "combined = rdd1.union(rdd2)\n",
    "\n",
    "print(f\"RDD1: {rdd1.collect()}\")\n",
    "print(f\"RDD2: {rdd2.collect()}\")\n",
    "print(f\"Union: {combined.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection() and subtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set operations\n",
    "rdd_a = sc.parallelize([1, 2, 3, 4, 5])\n",
    "rdd_b = sc.parallelize([3, 4, 5, 6, 7])\n",
    "\n",
    "# Intersection\n",
    "common = rdd_a.intersection(rdd_b)\n",
    "print(f\"Intersection: {common.collect()}\")\n",
    "\n",
    "# Subtract (elements in A but not in B)\n",
    "diff = rdd_a.subtract(rdd_b)\n",
    "print(f\"A - B: {diff.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartesian() - Cartesian product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cartesian product\n",
    "rdd_x = sc.parallelize([1, 2])\n",
    "rdd_y = sc.parallelize(['a', 'b'])\n",
    "product = rdd_x.cartesian(rdd_y)\n",
    "\n",
    "print(f\"Cartesian product: {product.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actions (Trigger Execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect() - Retrieve all elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all elements to driver (careful with large datasets!)\n",
    "data = sc.parallelize(range(1, 11))\n",
    "all_elements = data.collect()\n",
    "\n",
    "print(f\"All elements: {all_elements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count(), first(), take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count elements\n",
    "print(f\"Count: {data.count()}\")\n",
    "\n",
    "# Get first element\n",
    "print(f\"First: {data.first()}\")\n",
    "\n",
    "# Take n elements\n",
    "print(f\"First 3: {data.take(3)}\")\n",
    "\n",
    "# Take top n (requires ordering)\n",
    "print(f\"Top 3: {data.top(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce() - Aggregate elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum all numbers\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5])\n",
    "sum_all = numbers.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(f\"Sum: {sum_all}\")\n",
    "\n",
    "# Find maximum\n",
    "max_val = numbers.reduce(lambda a, b: a if a > b else b)\n",
    "print(f\"Max: {max_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fold() - Like reduce with initial value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold with initial value\n",
    "result = numbers.fold(0, lambda a, b: a + b)\n",
    "print(f\"Fold result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### foreach() - Apply function to each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each element (side effects)\n",
    "# Note: Output won't show in notebook, use for side effects like saving\n",
    "numbers.foreach(lambda x: print(f\"Processing: {x}\"))\n",
    "\n",
    "print(\"foreach completed (check executor logs for output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pair RDD Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair RDD (key-value pairs)\n",
    "pairs = sc.parallelize([\n",
    "    (\"apple\", 5),\n",
    "    (\"banana\", 3),\n",
    "    (\"apple\", 2),\n",
    "    (\"orange\", 4),\n",
    "    (\"banana\", 6)\n",
    "])\n",
    "\n",
    "print(f\"Pair RDD: {pairs.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keys() and values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys and values\n",
    "keys = pairs.keys()\n",
    "values = pairs.values()\n",
    "\n",
    "print(f\"Keys: {keys.collect()}\")\n",
    "print(f\"Values: {values.collect()}\")\n",
    "print(f\"Distinct keys: {keys.distinct().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey() - Aggregate by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum values by key\n",
    "totals = pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(f\"Totals by key: {totals.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey() - Group values by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all values for each key\n",
    "grouped = pairs.groupByKey()\n",
    "\n",
    "# Need to convert iterables to lists\n",
    "grouped_list = grouped.mapValues(list)\n",
    "\n",
    "print(f\"Grouped by key: {grouped_list.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapValues() - Transform values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double all values\n",
    "doubled = pairs.mapValues(lambda x: x * 2)\n",
    "\n",
    "print(f\"Original: {pairs.collect()}\")\n",
    "print(f\"Doubled values: {doubled.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey() - Sort by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by key\n",
    "sorted_asc = pairs.sortByKey(ascending=True)\n",
    "sorted_desc = pairs.sortByKey(ascending=False)\n",
    "\n",
    "print(f\"Sorted ascending: {sorted_asc.collect()}\")\n",
    "print(f\"Sorted descending: {sorted_desc.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join() operations on Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join pair RDDs\n",
    "prices = sc.parallelize([\n",
    "    (\"apple\", 0.5),\n",
    "    (\"banana\", 0.3),\n",
    "    (\"orange\", 0.6)\n",
    "])\n",
    "\n",
    "quantities = sc.parallelize([\n",
    "    (\"apple\", 10),\n",
    "    (\"banana\", 20),\n",
    "    (\"grape\", 15)\n",
    "])\n",
    "\n",
    "# Inner join\n",
    "inner_join = prices.join(quantities)\n",
    "print(f\"Inner join: {inner_join.collect()}\")\n",
    "\n",
    "# Left outer join\n",
    "left_join = prices.leftOuterJoin(quantities)\n",
    "print(f\"Left join: {left_join.collect()}\")\n",
    "\n",
    "# Right outer join\n",
    "right_join = prices.rightOuterJoin(quantities)\n",
    "print(f\"Right join: {right_join.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each key\n",
    "counts = pairs.countByKey()\n",
    "\n",
    "print(f\"Counts by key: {counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check partitions\n",
    "data = sc.parallelize(range(1, 101), 4)\n",
    "\n",
    "print(f\"Number of partitions: {data.getNumPartitions()}\")\n",
    "\n",
    "# View partition distribution\n",
    "partition_sizes = data.glom().map(len).collect()\n",
    "print(f\"Elements per partition: {partition_sizes}\")\n",
    "\n",
    "# Repartition\n",
    "repartitioned = data.repartition(8)\n",
    "print(f\"After repartition: {repartitioned.getNumPartitions()} partitions\")\n",
    "\n",
    "# Coalesce (reduce partitions)\n",
    "coalesced = data.coalesce(2)\n",
    "print(f\"After coalesce: {coalesced.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Partitioner for Pair RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by key (hash partitioning)\n",
    "pair_rdd = sc.parallelize([(i, i*2) for i in range(1, 21)])\n",
    "\n",
    "# Partition by key with 4 partitions\n",
    "partitioned_pairs = pair_rdd.partitionBy(4)\n",
    "\n",
    "print(f\"Partitioned by key: {partitioned_pairs.getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Persistence and Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Create RDD and cache it\n",
    "data = sc.parallelize(range(1, 1000))\n",
    "\n",
    "# Cache (memory only)\n",
    "cached_rdd = data.cache()\n",
    "\n",
    "# Or persist with specific storage level\n",
    "persisted_rdd = data.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Use cached RDD multiple times\n",
    "count1 = cached_rdd.count()\n",
    "count2 = cached_rdd.filter(lambda x: x > 500).count()\n",
    "\n",
    "print(f\"Total: {count1}, Greater than 500: {count2}\")\n",
    "\n",
    "# Unpersist when done\n",
    "cached_rdd.unpersist()\n",
    "persisted_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Word Count Example (Classic RDD Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic word count\n",
    "text = [\n",
    "    \"Apache Spark is fast\",\n",
    "    \"Spark is easy to use\",\n",
    "    \"Spark runs everywhere\"\n",
    "]\n",
    "\n",
    "text_rdd = sc.parallelize(text)\n",
    "\n",
    "# Word count pipeline\n",
    "word_counts = text_rdd \\\n",
    "    .flatMap(lambda line: line.lower().split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Word counts:\")\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Filter and Aggregate\n",
    "Given an RDD of numbers, find the sum of all even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "numbers = sc.parallelize(range(1, 101))\n",
    "# TODO: Filter evens and sum them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Pair RDD Operations\n",
    "Given sales data as (product, amount) pairs, find total sales per product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "sales = sc.parallelize([\n",
    "    (\"A\", 100), (\"B\", 200), (\"A\", 150), (\"C\", 300), (\"B\", 100)\n",
    "])\n",
    "# TODO: Calculate total per product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ RDD concepts and when to use them\n",
    "✅ Creating RDDs from various sources\n",
    "✅ Transformations (map, filter, flatMap, etc.)\n",
    "✅ Actions (collect, reduce, count, etc.)\n",
    "✅ Pair RDD operations (reduceByKey, join, etc.)\n",
    "✅ Partitioning strategies\n",
    "✅ Persistence and caching\n",
    "✅ Classic RDD patterns (word count)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Practice with more complex RDD operations\n",
    "2. Compare RDD vs DataFrame performance\n",
    "3. Learn when to use each API\n",
    "4. Study advanced partitioning strategies\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [PySpark RDD API](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html)\n",
    "- [Spark By Examples - RDD](https://sparkbyexamples.com/pyspark-rdd/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
