{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Lake Advanced Operations\n",
    "\n",
    "## Overview\n",
    "This notebook covers advanced Delta Lake features including MERGE, time travel, optimization, and schema evolution.\n",
    "\n",
    "## Learning Objectives\n",
    "- Perform MERGE (UPSERT) operations\n",
    "- Use time travel for auditing and rollback\n",
    "- Optimize Delta tables with OPTIMIZE and Z-ORDER\n",
    "- Manage schema evolution\n",
    "- Use VACUUM for cleanup\n",
    "- Work with Change Data Feed (CDF)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MERGE Operations (UPSERT)\n",
    "\n",
    "MERGE allows you to upsert (update + insert) data into Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create target table\n",
    "target_data = [\n",
    "    (1, \"Alice\", 25, \"2024-01-01\"),\n",
    "    (2, \"Bob\", 30, \"2024-01-01\"),\n",
    "    (3, \"Charlie\", 35, \"2024-01-01\")\n",
    "]\n",
    "\n",
    "target_df = spark.createDataFrame(\n",
    "    target_data,\n",
    "    [\"id\", \"name\", \"age\", \"updated_date\"]\n",
    ")\n",
    "\n",
    "# Write as Delta table\n",
    "target_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/users\")\n",
    "\n",
    "print(\"Initial target table:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/users\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create source data with updates and new records\n",
    "source_data = [\n",
    "    (2, \"Bob\", 31, \"2024-01-15\"),  # Update: age changed\n",
    "    (3, \"Charlie\", 35, \"2024-01-15\"),  # No change\n",
    "    (4, \"Diana\", 28, \"2024-01-15\")  # New record\n",
    "]\n",
    "\n",
    "source_df = spark.createDataFrame(\n",
    "    source_data,\n",
    "    [\"id\", \"name\", \"age\", \"updated_date\"]\n",
    ")\n",
    "\n",
    "print(\"Source data:\")\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform MERGE\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta/users\")\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"name\": \"source.name\",\n",
    "    \"age\": \"source.age\",\n",
    "    \"updated_date\": \"source.updated_date\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"id\": \"source.id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"age\": \"source.age\",\n",
    "    \"updated_date\": \"source.updated_date\"\n",
    "}).execute()\n",
    "\n",
    "print(\"After MERGE:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/users\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional MERGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE with conditions\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(\n",
    "    condition = \"source.age > target.age\",  # Only update if age increased\n",
    "    set = {\n",
    "        \"age\": \"source.age\",\n",
    "        \"updated_date\": \"source.updated_date\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"id\": \"source.id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"age\": \"source.age\",\n",
    "        \"updated_date\": \"source.updated_date\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"Conditional MERGE complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE with Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE that can also delete records\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedDelete(\n",
    "    condition = \"source.age < 30\"  # Delete if age < 30\n",
    ").whenMatchedUpdate(\n",
    "    set = {\n",
    "        \"age\": \"source.age\",\n",
    "        \"updated_date\": \"source.updated_date\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values = {\n",
    "        \"id\": \"source.id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"age\": \"source.age\",\n",
    "        \"updated_date\": \"source.updated_date\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "print(\"MERGE with delete complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Travel\n",
    "\n",
    "Query previous versions of your Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View history\n",
    "delta_table = DeltaTable.forPath(spark, \"/tmp/delta/users\")\n",
    "history_df = delta_table.history()\n",
    "\n",
    "print(\"Table history:\")\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query by version\n",
    "version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta/users\")\n",
    "\n",
    "print(\"Version 0 (original data):\")\n",
    "version_0.show()\n",
    "\n",
    "# Query by timestamp\n",
    "# df_timestamp = spark.read.format(\"delta\") \\\n",
    "#     .option(\"timestampAsOf\", \"2024-01-01\") \\\n",
    "#     .load(\"/tmp/delta/users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL time travel\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW users_current \n",
    "    USING DELTA \n",
    "    LOCATION '/tmp/delta/users'\n",
    "\"\"\")\n",
    "\n",
    "# Query specific version in SQL\n",
    "spark.sql(\"SELECT * FROM users_current VERSION AS OF 0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore to Previous Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore to version 0\n",
    "# delta_table.restoreToVersion(0)\n",
    "\n",
    "# Or restore to timestamp\n",
    "# delta_table.restoreToTimestamp(\"2024-01-01\")\n",
    "\n",
    "print(\"Restore operations available (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OPTIMIZE and Z-ORDER\n",
    "\n",
    "Optimize file layout for better query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger table for optimization demo\n",
    "large_data = [(i, f\"User{i}\", 20 + (i % 30), \"2024-01-01\") for i in range(1, 10001)]\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"name\", \"age\", \"date\"])\n",
    "\n",
    "# Write with many small files\n",
    "large_df.repartition(100).write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/large_users\")\n",
    "\n",
    "# Check file stats before optimization\n",
    "print(\"Before OPTIMIZE:\")\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`/tmp/delta/large_users`\").select(\"numFiles\", \"sizeInBytes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE - compact small files\n",
    "spark.sql(\"OPTIMIZE delta.`/tmp/delta/large_users`\")\n",
    "\n",
    "print(\"\\nAfter OPTIMIZE:\")\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`/tmp/delta/large_users`\").select(\"numFiles\", \"sizeInBytes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE with Z-ORDER\n",
    "# Z-ORDER clusters data by specified columns for better filtering\n",
    "spark.sql(\"OPTIMIZE delta.`/tmp/delta/large_users` ZORDER BY (age)\")\n",
    "\n",
    "print(\"OPTIMIZE with Z-ORDER complete\")\n",
    "print(\"Queries filtering on 'age' will now be faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial table\n",
    "initial_data = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "initial_df = spark.createDataFrame(initial_data, [\"id\", \"name\"])\n",
    "initial_df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta/schema_test\")\n",
    "\n",
    "print(\"Initial schema:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/schema_test\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to add new column - will fail without mergeSchema\n",
    "new_data = [(3, \"Charlie\", 25)]\n",
    "new_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"age\"])\n",
    "\n",
    "# This would fail:\n",
    "# new_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/schema_test\")\n",
    "\n",
    "# Enable schema evolution\n",
    "new_df.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"/tmp/delta/schema_test\")\n",
    "\n",
    "print(\"\\nAfter schema evolution:\")\n",
    "evolved_df = spark.read.format(\"delta\").load(\"/tmp/delta/schema_test\")\n",
    "evolved_df.printSchema()\n",
    "evolved_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic schema merging\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# Now schema evolution happens automatically\n",
    "another_col_data = [(4, \"Diana\", 28, \"Engineer\")]\n",
    "another_df = spark.createDataFrame(another_col_data, [\"id\", \"name\", \"age\", \"title\"])\n",
    "\n",
    "another_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/schema_test\")\n",
    "\n",
    "print(\"After auto schema evolution:\")\n",
    "spark.read.format(\"delta\").load(\"/tmp/delta/schema_test\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. VACUUM - Clean Up Old Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM removes old files no longer referenced\n",
    "# Default retention: 7 days\n",
    "\n",
    "# Check what will be deleted (dry run)\n",
    "spark.sql(\"VACUUM delta.`/tmp/delta/users` RETAIN 0 HOURS DRY RUN\").show()\n",
    "\n",
    "# To actually vacuum (need to disable safety check for < 7 days)\n",
    "# spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "# spark.sql(\"VACUUM delta.`/tmp/delta/users` RETAIN 0 HOURS\")\n",
    "\n",
    "print(\"VACUUM dry run complete\")\n",
    "print(\"⚠️ Be careful with VACUUM - it deletes old files permanently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Change Data Feed (CDF)\n",
    "\n",
    "Track row-level changes in Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table with CDF enabled\n",
    "cdf_data = [(1, \"Alice\", 100), (2, \"Bob\", 200)]\n",
    "cdf_df = spark.createDataFrame(cdf_data, [\"id\", \"name\", \"amount\"])\n",
    "\n",
    "cdf_df.write.format(\"delta\") \\\n",
    "    .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/cdf_table\")\n",
    "\n",
    "print(\"Table with CDF enabled created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some changes\n",
    "updates = [(1, \"Alice\", 150), (3, \"Charlie\", 300)]\n",
    "update_df = spark.createDataFrame(updates, [\"id\", \"name\", \"amount\"])\n",
    "\n",
    "delta_cdf = DeltaTable.forPath(spark, \"/tmp/delta/cdf_table\")\n",
    "delta_cdf.alias(\"target\").merge(\n",
    "    update_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"amount\": \"source.amount\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"id\": \"source.id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"amount\": \"source.amount\"\n",
    "}).execute()\n",
    "\n",
    "print(\"Changes made\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read change data feed\n",
    "changes = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeData\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .load(\"/tmp/delta/cdf_table\")\n",
    "\n",
    "print(\"Change Data Feed:\")\n",
    "changes.select(\"id\", \"name\", \"amount\", \"_change_type\", \"_commit_version\").show()\n",
    "\n",
    "# _change_type values:\n",
    "# - insert: new row\n",
    "# - update_preimage: old value before update\n",
    "# - update_postimage: new value after update\n",
    "# - delete: deleted row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Constraints and Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CHECK constraint\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE delta.`/tmp/delta/users`\n",
    "    ADD CONSTRAINT age_positive CHECK (age > 0)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Constraint added: age must be positive\")\n",
    "\n",
    "# This would fail:\n",
    "# bad_data = [(10, \"Invalid\", -5, \"2024-01-01\")]\n",
    "# bad_df = spark.createDataFrame(bad_data, [\"id\", \"name\", \"age\", \"updated_date\"])\n",
    "# bad_df.write.format(\"delta\").mode(\"append\").save(\"/tmp/delta/users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Table Properties and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table details\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`/tmp/delta/users`\").show(vertical=True)\n",
    "\n",
    "# View table properties\n",
    "spark.sql(\"SHOW TBLPROPERTIES delta.`/tmp/delta/users`\").show()\n",
    "\n",
    "# Set table properties\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE delta.`/tmp/delta/users`\n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.logRetentionDuration' = '30 days',\n",
    "        'delta.deletedFileRetentionDuration' = '7 days'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table properties updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement SCD Type 2\n",
    "Create a Slowly Changing Dimension Type 2 table using MERGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Implement SCD Type 2 with effective_date and end_date columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimize for Query Performance\n",
    "Given a large table, optimize it for queries filtering by date and product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Use OPTIMIZE with Z-ORDER on appropriate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ MERGE operations for UPSERT patterns\n",
    "✅ Time travel for auditing and rollback\n",
    "✅ OPTIMIZE and Z-ORDER for performance\n",
    "✅ Schema evolution techniques\n",
    "✅ VACUUM for cleanup\n",
    "✅ Change Data Feed for CDC\n",
    "✅ Constraints and data quality\n",
    "✅ Table properties and metadata\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Practice MERGE patterns for real scenarios\n",
    "2. Implement CDC pipelines with CDF\n",
    "3. Learn about partition pruning\n",
    "4. Study Delta Lake internals\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Delta Lake Guide](https://docs.delta.io/)\n",
    "- [Databricks Delta Lake](https://docs.databricks.com/delta/index.html)\n",
    "- [Spark By Examples - Delta Lake](https://sparkbyexamples.com/spark/spark-delta-lake-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
