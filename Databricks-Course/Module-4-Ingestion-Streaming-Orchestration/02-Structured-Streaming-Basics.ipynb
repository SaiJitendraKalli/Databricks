{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Basics\n",
    "\n",
    "## Overview\n",
    "This notebook introduces Structured Streaming in Spark - a scalable and fault-tolerant stream processing engine built on Spark SQL.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand streaming concepts\n",
    "- Read from streaming sources\n",
    "- Apply transformations on streams\n",
    "- Write to streaming sinks\n",
    "- Understand triggers and output modes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Concepts\n",
    "\n",
    "### What is Streaming?\n",
    "\n",
    "**Batch Processing**:\n",
    "```\n",
    "Data → Process → Results\n",
    "      (once)\n",
    "```\n",
    "\n",
    "**Stream Processing**:\n",
    "```\n",
    "Data Stream → Process → Results Stream\n",
    "             (continuously)\n",
    "```\n",
    "\n",
    "### Structured Streaming Model\n",
    "\n",
    "Think of streaming data as an **unbounded table** that continuously grows:\n",
    "\n",
    "```\n",
    "Input Stream → Unbounded Table → Query → Result Table → Output\n",
    "```\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Source**: Where data comes from (Kafka, files, sockets)\n",
    "- **Transformation**: Operations on streaming data\n",
    "- **Sink**: Where results go (console, files, Delta tables)\n",
    "- **Trigger**: When to process new data\n",
    "- **Checkpoint**: For fault tolerance and exactly-once semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Streams\n",
    "\n",
    "### From Files (Most Common in Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define schema for streaming data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"value\", DoubleType(), False),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read stream from CSV files\n",
    "stream_df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/path/to/streaming/directory\")\n",
    "\n",
    "print(\"Stream DataFrame created (not executed yet)\")\n",
    "print(f\"Is streaming: {stream_df.isStreaming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Delta Tables (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stream from Delta table\n",
    "delta_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(\"/path/to/delta/table\")\n",
    "\n",
    "# Read with options\n",
    "delta_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 10) \\\n",
    "    .option(\"startingVersion\", \"latest\") \\\n",
    "    .load(\"/path/to/delta/table\")\n",
    "\n",
    "print(\"Delta stream configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from Kafka topic\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Kafka data comes as binary - need to cast\n",
    "kafka_data = kafka_stream.select(\n",
    "    col(\"key\").cast(\"string\"),\n",
    "    col(\"value\").cast(\"string\"),\n",
    "    col(\"topic\"),\n",
    "    col(\"partition\"),\n",
    "    col(\"offset\"),\n",
    "    col(\"timestamp\")\n",
    ")\n",
    "\n",
    "print(\"Kafka stream configured (requires Kafka cluster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stream Transformations\n",
    "\n",
    "Most DataFrame operations work on streams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have stream_df from earlier\n",
    "\n",
    "# Filter\n",
    "filtered_stream = stream_df.filter(col(\"value\") > 100)\n",
    "\n",
    "# Select and transform\n",
    "transformed_stream = stream_df.select(\n",
    "    col(\"id\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"value\"),\n",
    "    (col(\"value\") * 1.1).alias(\"value_with_tax\"),\n",
    "    upper(col(\"category\")).alias(\"category_upper\")\n",
    ")\n",
    "\n",
    "# Add derived columns\n",
    "enriched_stream = stream_df \\\n",
    "    .withColumn(\"hour\", hour(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"date\", to_date(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"value_category\",\n",
    "                when(col(\"value\") < 50, \"Low\")\n",
    "                .when(col(\"value\") < 100, \"Medium\")\n",
    "                .otherwise(\"High\"))\n",
    "\n",
    "print(\"Stream transformations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregations on Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple aggregation (running count)\n",
    "count_stream = stream_df \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .count()\n",
    "\n",
    "# Multiple aggregations\n",
    "stats_stream = stream_df \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"value\").alias(\"avg_value\"),\n",
    "        sum(\"value\").alias(\"total_value\"),\n",
    "        min(\"value\").alias(\"min_value\"),\n",
    "        max(\"value\").alias(\"max_value\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregation streams defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-based Aggregations (Windowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tumbling window (non-overlapping)\n",
    "tumbling_window_stream = stream_df \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 minutes\"),\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"value\").alias(\"avg_value\")\n",
    "    )\n",
    "\n",
    "# Sliding window (overlapping)\n",
    "sliding_window_stream = stream_df \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\"),  # 10min window, 5min slide\n",
    "        col(\"category\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "print(\"Window aggregations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing Streams\n",
    "\n",
    "### Output Modes\n",
    "\n",
    "- **Append**: Only new rows (default for non-aggregated)\n",
    "- **Complete**: Entire result table (for aggregations)\n",
    "- **Update**: Only changed rows (for aggregations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Console (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to console\n",
    "query = stream_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for termination (would run continuously)\n",
    "# query.awaitTermination()\n",
    "\n",
    "# Stop the query\n",
    "# query.stop()\n",
    "\n",
    "print(\"Console query created (commented out to avoid running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Delta Table (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stream to Delta table\n",
    "delta_query = stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .start(\"/path/to/output/delta/table\")\n",
    "\n",
    "# With partitioning\n",
    "partitioned_query = stream_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .start(\"/path/to/output/delta/table\")\n",
    "\n",
    "print(\"Delta write queries configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet files\n",
    "file_query = stream_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .option(\"path\", \"/path/to/output\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"File write query configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Triggers\n",
    "\n",
    "Control when processing happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default trigger (process ASAP)\n",
    "default_query = stream_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Fixed interval trigger (micro-batch every X time)\n",
    "timed_query = stream_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# One-time trigger (process once and stop)\n",
    "once_query = stream_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "# Available batch trigger (process all available data)\n",
    "available_query = stream_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Various trigger types demonstrated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Managing Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have a query running\n",
    "# query = stream_df.writeStream.format(\"console\").start()\n",
    "\n",
    "# Get query ID\n",
    "# print(f\"Query ID: {query.id}\")\n",
    "\n",
    "# Get query name\n",
    "# print(f\"Query name: {query.name}\")\n",
    "\n",
    "# Check if query is active\n",
    "# print(f\"Is active: {query.isActive}\")\n",
    "\n",
    "# Get recent progress\n",
    "# print(query.recentProgress)\n",
    "\n",
    "# Get last progress\n",
    "# print(query.lastProgress)\n",
    "\n",
    "# Stop query\n",
    "# query.stop()\n",
    "\n",
    "# List all active streams\n",
    "# print(spark.streams.active)\n",
    "\n",
    "# Await termination with timeout\n",
    "# query.awaitTermination(timeout=60)  # Wait up to 60 seconds\n",
    "\n",
    "print(\"Query management methods (commented out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Example: Real-time Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Real-time sales monitoring\n",
    "\n",
    "# 1. Define schema\n",
    "sales_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"product_id\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"quantity\", IntegerType(), False),\n",
    "    StructField(\"region\", StringType(), False)\n",
    "])\n",
    "\n",
    "# 2. Read stream\n",
    "sales_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .schema(sales_schema) \\\n",
    "    .load(\"/bronze/sales\")\n",
    "\n",
    "# 3. Transform and aggregate\n",
    "sales_summary = sales_stream \\\n",
    "    .withColumn(\"revenue\", col(\"amount\") * col(\"quantity\")) \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\"),\n",
    "        col(\"region\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"revenue\").alias(\"total_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction_amount\")\n",
    "    )\n",
    "\n",
    "# 4. Write to Delta table\n",
    "sales_query = sales_summary.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/sales_summary\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .start(\"/gold/sales_summary\")\n",
    "\n",
    "print(\"Complete streaming pipeline configured\")\n",
    "print(f\"Query name: {sales_query.name}\")\n",
    "\n",
    "# In production, you would:\n",
    "# sales_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### Checkpoint Location\n",
    "Always specify checkpoint location for fault tolerance:\n",
    "```python\n",
    ".option(\"checkpointLocation\", \"/reliable/path\")\n",
    "```\n",
    "\n",
    "### Schema Definition\n",
    "Always define schema explicitly for streams:\n",
    "```python\n",
    ".schema(my_schema)\n",
    "```\n",
    "\n",
    "### Output Mode Selection\n",
    "- Use **append** for non-aggregated data\n",
    "- Use **update** for aggregations (most efficient)\n",
    "- Use **complete** only when necessary (outputs entire table)\n",
    "\n",
    "### Monitoring\n",
    "- Use `query.recentProgress` to monitor\n",
    "- Check `inputRowsPerSecond` and `processedRowsPerSecond`\n",
    "- Monitor checkpoint size\n",
    "\n",
    "### Error Handling\n",
    "- Streams will automatically retry on transient failures\n",
    "- Monitor for persistent failures\n",
    "- Use dead letter queues for bad records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Common Patterns\n",
    "\n",
    "### Pattern 1: Bronze to Silver Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from bronze\n",
    "bronze_stream = spark.readStream.format(\"delta\").load(\"/bronze/raw_events\")\n",
    "\n",
    "# Clean and enrich\n",
    "silver_stream = bronze_stream \\\n",
    "    .filter(col(\"event_id\").isNotNull()) \\\n",
    "    .dropDuplicates([\"event_id\"]) \\\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"date\", to_date(col(\"timestamp\")))\n",
    "\n",
    "# Write to silver\n",
    "silver_query = silver_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .start(\"/silver/clean_events\")\n",
    "\n",
    "print(\"Bronze to Silver pattern configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Stream-Stream Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two streams\n",
    "orders_stream = spark.readStream.format(\"delta\").load(\"/streams/orders\")\n",
    "payments_stream = spark.readStream.format(\"delta\").load(\"/streams/payments\")\n",
    "\n",
    "# Join with watermark\n",
    "joined_stream = orders_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"10 minutes\") \\\n",
    "    .join(\n",
    "        payments_stream.withWatermark(\"payment_timestamp\", \"10 minutes\"),\n",
    "        \"order_id\",\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "print(\"Stream-stream join pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Streaming concepts and architecture\n",
    "✅ Reading from various streaming sources\n",
    "✅ Applying transformations on streams\n",
    "✅ Windowed aggregations\n",
    "✅ Writing to different sinks\n",
    "✅ Triggers and output modes\n",
    "✅ Managing streaming queries\n",
    "✅ Best practices and common patterns\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Practice with real streaming data\n",
    "2. Learn about watermarking for late data\n",
    "3. Explore Delta Live Tables for declarative pipelines\n",
    "4. Study stateful operations\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Databricks Streaming](https://docs.databricks.com/structured-streaming/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
