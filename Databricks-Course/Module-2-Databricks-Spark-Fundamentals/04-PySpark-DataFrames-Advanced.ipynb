{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames Advanced Operations\n",
    "\n",
    "## Overview\n",
    "This notebook covers advanced DataFrame operations including window functions, pivoting, unpivoting, and complex transformations.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master window functions for analytics\n",
    "- Perform pivoting and unpivoting\n",
    "- Use advanced column operations\n",
    "- Handle complex data types (arrays, structs, maps)\n",
    "- Apply User Defined Functions (UDFs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Window Functions\n",
    "\n",
    "Window functions perform calculations across rows related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Sample data\n",
    "sales_data = [\n",
    "    (\"2024-01-01\", \"Electronics\", \"Laptop\", 1200, 2),\n",
    "    (\"2024-01-02\", \"Electronics\", \"Phone\", 800, 5),\n",
    "    (\"2024-01-03\", \"Clothing\", \"Shirt\", 50, 10),\n",
    "    (\"2024-01-04\", \"Electronics\", \"Tablet\", 400, 3),\n",
    "    (\"2024-01-05\", \"Clothing\", \"Pants\", 80, 8),\n",
    "    (\"2024-01-06\", \"Electronics\", \"Laptop\", 1200, 1),\n",
    "    (\"2024-01-07\", \"Clothing\", \"Jacket\", 150, 4)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"date\", \"category\", \"product\", \"price\", \"quantity\"]\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"date\", to_date(col(\"date\")))\n",
    "df = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROW_NUMBER, RANK, DENSE_RANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(col(\"revenue\").desc())\n",
    "\n",
    "# Apply ranking functions\n",
    "df_ranked = df.select(\n",
    "    \"date\",\n",
    "    \"category\",\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    row_number().over(window_spec).alias(\"row_num\"),\n",
    "    rank().over(window_spec).alias(\"rank\"),\n",
    "    dense_rank().over(window_spec).alias(\"dense_rank\")\n",
    ")\n",
    "\n",
    "display(df_ranked.orderBy(\"category\", \"row_num\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAG and LEAD Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window by category, ordered by date\n",
    "date_window = Window.partitionBy(\"category\").orderBy(\"date\")\n",
    "\n",
    "df_lag_lead = df.select(\n",
    "    \"date\",\n",
    "    \"category\",\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    lag(\"revenue\", 1).over(date_window).alias(\"prev_revenue\"),\n",
    "    lead(\"revenue\", 1).over(date_window).alias(\"next_revenue\"),\n",
    "    (col(\"revenue\") - lag(\"revenue\", 1).over(date_window)).alias(\"revenue_change\")\n",
    ")\n",
    "\n",
    "display(df_lag_lead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Totals and Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running total\n",
    "running_total_window = Window.partitionBy(\"category\").orderBy(\"date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Moving average (3-day window)\n",
    "moving_avg_window = Window.partitionBy(\"category\").orderBy(\"date\") \\\n",
    "    .rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "df_windowed = df.select(\n",
    "    \"date\",\n",
    "    \"category\",\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    sum(\"revenue\").over(running_total_window).alias(\"running_total\"),\n",
    "    avg(\"revenue\").over(moving_avg_window).alias(\"moving_avg_3day\"),\n",
    "    count(\"*\").over(running_total_window).alias(\"cumulative_count\")\n",
    ")\n",
    "\n",
    "display(df_windowed.orderBy(\"category\", \"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTILE for Quartiles/Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into quartiles by revenue\n",
    "quartile_window = Window.orderBy(col(\"revenue\").desc())\n",
    "\n",
    "df_quartiles = df.select(\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    ntile(4).over(quartile_window).alias(\"quartile\"),\n",
    "    percent_rank().over(quartile_window).alias(\"percent_rank\")\n",
    ")\n",
    "\n",
    "display(df_quartiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pivot and Unpivot Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot - Convert Rows to Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: Category revenue by date\n",
    "df_pivot = df.groupBy(\"date\").pivot(\"category\").sum(\"revenue\")\n",
    "\n",
    "display(df_pivot.orderBy(\"date\"))\n",
    "\n",
    "# Pivot with specific values (more efficient)\n",
    "df_pivot_opt = df.groupBy(\"date\") \\\n",
    "    .pivot(\"category\", [\"Electronics\", \"Clothing\"]) \\\n",
    "    .agg(sum(\"revenue\").alias(\"revenue\"))\n",
    "\n",
    "display(df_pivot_opt.orderBy(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpivot - Convert Columns to Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivoted dataframe first\n",
    "pivoted_df = df.groupBy(\"date\").pivot(\"category\").sum(\"revenue\")\n",
    "\n",
    "# Unpivot using stack\n",
    "df_unpivot = pivoted_df.select(\n",
    "    \"date\",\n",
    "    expr(\"stack(2, 'Electronics', Electronics, 'Clothing', Clothing) as (category, revenue)\")\n",
    ")\n",
    "\n",
    "display(df_unpivot.orderBy(\"date\", \"category\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Complex Data Types\n",
    "\n",
    "### Working with Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array columns\n",
    "df_arrays = df.select(\n",
    "    \"product\",\n",
    "    \"category\",\n",
    "    array(\"price\", \"quantity\").alias(\"metrics\"),\n",
    "    split(col(\"product\"), \"\").alias(\"product_chars\")\n",
    ")\n",
    "\n",
    "display(df_arrays)\n",
    "\n",
    "# Array operations\n",
    "df_array_ops = df_arrays.select(\n",
    "    \"product\",\n",
    "    \"metrics\",\n",
    "    size(\"metrics\").alias(\"array_size\"),\n",
    "    array_contains(\"metrics\", 1200).alias(\"has_1200\"),\n",
    "    element_at(\"metrics\", 1).alias(\"first_element\"),\n",
    "    sort_array(\"metrics\").alias(\"sorted_metrics\")\n",
    ")\n",
    "\n",
    "display(df_array_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode array into rows\n",
    "df_exploded = df_arrays.select(\n",
    "    \"product\",\n",
    "    \"category\",\n",
    "    explode(\"metrics\").alias(\"metric_value\")\n",
    ")\n",
    "\n",
    "display(df_exploded)\n",
    "\n",
    "# Explode with position\n",
    "df_posexplode = df_arrays.select(\n",
    "    \"product\",\n",
    "    posexplode(\"metrics\").alias(\"pos\", \"value\")\n",
    ")\n",
    "\n",
    "display(df_posexplode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create struct column\n",
    "df_struct = df.select(\n",
    "    \"product\",\n",
    "    struct(\n",
    "        col(\"price\").alias(\"unit_price\"),\n",
    "        col(\"quantity\"),\n",
    "        col(\"revenue\")\n",
    "    ).alias(\"sale_info\")\n",
    ")\n",
    "\n",
    "display(df_struct)\n",
    "\n",
    "# Access struct fields\n",
    "df_struct_access = df_struct.select(\n",
    "    \"product\",\n",
    "    \"sale_info\",\n",
    "    col(\"sale_info.unit_price\").alias(\"price\"),\n",
    "    col(\"sale_info.quantity\").alias(\"qty\")\n",
    ")\n",
    "\n",
    "display(df_struct_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map column\n",
    "df_map = df.select(\n",
    "    \"product\",\n",
    "    create_map(\n",
    "        lit(\"price\"), col(\"price\"),\n",
    "        lit(\"quantity\"), col(\"quantity\").cast(\"double\")\n",
    "    ).alias(\"attributes\")\n",
    ")\n",
    "\n",
    "display(df_map)\n",
    "\n",
    "# Access map values\n",
    "df_map_access = df_map.select(\n",
    "    \"product\",\n",
    "    col(\"attributes\")[\"price\"].alias(\"price_from_map\"),\n",
    "    map_keys(\"attributes\").alias(\"keys\"),\n",
    "    map_values(\"attributes\").alias(\"values\")\n",
    ")\n",
    "\n",
    "display(df_map_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. JSON Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample JSON data\n",
    "json_data = [\n",
    "    (1, '{\"name\":\"Alice\",\"age\":25,\"city\":\"NY\"}'),\n",
    "    (2, '{\"name\":\"Bob\",\"age\":30,\"city\":\"LA\"}'),\n",
    "    (3, '{\"name\":\"Charlie\",\"age\":35,\"city\":\"SF\"}')\n",
    "]\n",
    "\n",
    "df_json = spark.createDataFrame(json_data, [\"id\", \"json_str\"])\n",
    "\n",
    "# Parse JSON\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"city\", StringType())\n",
    "])\n",
    "\n",
    "df_parsed = df_json.select(\n",
    "    \"id\",\n",
    "    from_json(col(\"json_str\"), schema).alias(\"data\")\n",
    ").select(\n",
    "    \"id\",\n",
    "    \"data.*\"\n",
    ")\n",
    "\n",
    "display(df_parsed)\n",
    "\n",
    "# Convert to JSON\n",
    "df_to_json = df_parsed.select(\n",
    "    \"id\",\n",
    "    to_json(struct(\"name\", \"age\", \"city\")).alias(\"json_output\")\n",
    ")\n",
    "\n",
    "display(df_to_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Simple UDF\n",
    "def categorize_price(price):\n",
    "    if price < 100:\n",
    "        return \"Low\"\n",
    "    elif price < 500:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "# Register UDF\n",
    "categorize_udf = udf(categorize_price, StringType())\n",
    "\n",
    "# Use UDF\n",
    "df_with_udf = df.select(\n",
    "    \"product\",\n",
    "    \"price\",\n",
    "    categorize_udf(col(\"price\")).alias(\"price_category\")\n",
    ")\n",
    "\n",
    "display(df_with_udf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF (More Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# Pandas UDF for scalar operations\n",
    "@pandas_udf(DoubleType())\n",
    "def calculate_discount(price: pd.Series) -> pd.Series:\n",
    "    return price * 0.9  # 10% discount\n",
    "\n",
    "df_discount = df.select(\n",
    "    \"product\",\n",
    "    \"price\",\n",
    "    calculate_discount(col(\"price\")).alias(\"discounted_price\")\n",
    ")\n",
    "\n",
    "display(df_discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String operations\n",
    "text_df = spark.createDataFrame(\n",
    "    [(\"  Hello World  \",), (\"PySpark Tutorial\",), (\"Data Engineering\",)],\n",
    "    [\"text\"]\n",
    ")\n",
    "\n",
    "df_strings = text_df.select(\n",
    "    \"text\",\n",
    "    upper(\"text\").alias(\"upper\"),\n",
    "    lower(\"text\").alias(\"lower\"),\n",
    "    trim(\"text\").alias(\"trimmed\"),\n",
    "    ltrim(\"text\").alias(\"ltrimmed\"),\n",
    "    rtrim(\"text\").alias(\"rtrimmed\"),\n",
    "    length(\"text\").alias(\"length\"),\n",
    "    substring(\"text\", 1, 5).alias(\"first_5_chars\"),\n",
    "    regexp_replace(\"text\", \"\\\\s+\", \"_\").alias(\"replace_spaces\"),\n",
    "    regexp_extract(\"text\", \"(\\\\w+)\", 1).alias(\"first_word\")\n",
    ")\n",
    "\n",
    "display(df_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Date and Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date operations\n",
    "df_dates = df.select(\n",
    "    \"date\",\n",
    "    year(\"date\").alias(\"year\"),\n",
    "    month(\"date\").alias(\"month\"),\n",
    "    dayofmonth(\"date\").alias(\"day\"),\n",
    "    dayofweek(\"date\").alias(\"day_of_week\"),\n",
    "    dayofyear(\"date\").alias(\"day_of_year\"),\n",
    "    weekofyear(\"date\").alias(\"week_of_year\"),\n",
    "    quarter(\"date\").alias(\"quarter\"),\n",
    "    date_format(\"date\", \"yyyy-MM\").alias(\"year_month\"),\n",
    "    date_add(\"date\", 7).alias(\"plus_7_days\"),\n",
    "    date_sub(\"date\", 7).alias(\"minus_7_days\"),\n",
    "    datediff(current_date(), \"date\").alias(\"days_since\")\n",
    ")\n",
    "\n",
    "display(df_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conditional Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when/otherwise\n",
    "df_conditional = df.select(\n",
    "    \"product\",\n",
    "    \"revenue\",\n",
    "    when(col(\"revenue\") > 2000, \"High\")\n",
    "        .when(col(\"revenue\") > 500, \"Medium\")\n",
    "        .otherwise(\"Low\")\n",
    "        .alias(\"revenue_tier\"),\n",
    "    \n",
    "    # Multiple conditions\n",
    "    when((col(\"revenue\") > 1000) & (col(\"category\") == \"Electronics\"), \"Premium Electronics\")\n",
    "        .when(col(\"revenue\") > 1000, \"Premium Other\")\n",
    "        .otherwise(\"Standard\")\n",
    "        .alias(\"segment\")\n",
    ")\n",
    "\n",
    "display(df_conditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Top Products per Category\n",
    "Find the top 2 products by revenue in each category using window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Use row_number() with window to get top 2 per category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Moving Average\n",
    "Calculate a 3-day moving average of revenue for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "# TODO: Use window with rowsBetween for moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "✅ Window functions (row_number, rank, lag, lead)\n",
    "✅ Running totals and moving averages\n",
    "✅ Pivot and unpivot operations\n",
    "✅ Complex data types (arrays, structs, maps)\n",
    "✅ JSON operations\n",
    "✅ User Defined Functions (UDFs)\n",
    "✅ Advanced string and date operations\n",
    "✅ Conditional expressions\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete the practice exercises\n",
    "2. Explore more PySpark functions\n",
    "3. Learn about joins and performance optimization\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [PySpark Functions API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [Spark By Examples](https://sparkbyexamples.com/pyspark-tutorial/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
